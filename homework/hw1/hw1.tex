\documentclass{article}
\input{../homework.sty}

\title{Homework 1}
\author{Austin Gill}

\begin{document}
\maketitle
\begingroup
\hypersetup{linkcolor=black}
\tableofcontents
% TODO: Remove this.
\listoftodos
\endgroup
\newpage

The Jupyter notebooks containing the work for each of the following problems, as well as the
\LaTeX{} source code for this document can be found at
\url{https://github.com/Notgnoshi/natural-computing/tree/master/homework/hw1}

\section{Function Optimization via Simulated Annealing}\label{prob:1}

\subsection{Statement}
Compare the effectiveness of the text's iterated hill climbing and simulated annealing algorithms
to find the max of
\[ f(x) = 2^{-2{\left(\frac{(x - 0.1)}{0.9}\right)}^2}{\big(\sin(5\pi x)\big)}^6\]
with $x\in [0,1]$. Use a real valued representation. Include a plot of the function with the
location of the max and a plot of the estimate as a function of the iteration number. How sensitive
are the algorithms to initial values?

\subsection{Method}

\subsubsection{Problem Setup and Gradient Methods}

For completeness and curiosity, I chose to attack the problem from a symbolic perspective first.
This gives an opportunity to get comfortable with the problem, and get my workflow setup correctly
because I've recently reinstalled Ubuntu and subsequently all of my commonly used Python libraries.

The python \mintinline{python}{sympy} library provides decent symbolic computation capabilities
that I have had good luck with in the past. The only real surprising implementation detail is in
the definition of the objective function.

\begin{minted}[highlightlines={7-8}]{python}
def f(x):
    """The objective function to evaluate.

    This function returns a sympy symbolic function, float, or np.ndarray
    depending on the type of the input.
    """
    # Use symbolic sine, pi if necessary.
    sin, pi = (sympy.sin, sympy.pi) if isinstance(x, sympy.Symbol) else (np.sin, np.pi)

    return 2 ** (-2 * ((x - 0.1) / 0.9) ** 2) * sin(5 * pi * x) ** 6
\end{minted}

If the given type is a symbolic variable, return a symbolic representation of the function rather
than the default numerical computation. Then this function can be given individual float values,
numpy arrays, and sympy symbols.

\begin{figure}[h]
    \centering
    % Make sure to run the notebook before this will compile.
    \includegraphics{prob1/figures/prob1-function.pdf}
    \caption{The objective function $f(x)$}\label{fig:prob1:function}
\end{figure}

The very first thing to do when dealing with optimizing a function is to plot anything you can get
your hands on, including the neighbor's cat. The objective function and its derivative are shown in
\autoref{fig:prob1:function} and \autoref{fig:prob1:derivative} respectively.

\begin{figure}[h]
    \centering
    \includegraphics{prob1/figures/prob1-derivative.pdf}
    \caption{The objective function's derivative}\label{fig:prob1:derivative}
\end{figure}

We can immediately pick out the local and global extrema by visual inspection. The maxima
periodically occur at $\frac{n}{5} - \frac{1}{10}$ with the global maximum occuring at
$\frac{1}{10}$. The minima periodically occur at $\frac{n}{5}$ with value $0$.

We can produce a symbolic version of the objective function easily with
\begin{minted}{python}
    x = sympy.Symbol('x')
    print(f(x))
\end{minted}
which displays\footnote{When coerced with \mintinline{python}{sympy.init_printing()} in a Jupyter
    notebook.}
\[2^{- 2 {\left(1.11 x - 0.11\right)}^{2}} \sin^{6}{\left (5 \pi x
        \right)}\]
Then we compute the atrocious derivative symbolically.
\begin{minted}{python}
    fp = sympy.diff(f(x), x)
    print(fp)
\end{minted}
which displays the trivial derivative\footnote{Coincidentally, the programmatically generated, and
    subsequently auto-formatted \LaTeX{} code for these functions is, I think, the ugliest thing
    I've ever seen.}
% This garbage is what sympy + my autoformatter gives. I'm not fixing it.
\[2^{- 2 \left(1.11 x - 0.11\right)^{2}} \left(- 4.938 x +
    0.4938\right) \log{\left (2 \right )} \sin^{6}{\left (5 \pi x \right )} + 30 \cdot
    2^{-2
            \left(1.11 x - 0.11\right)^{2}} \pi \sin^{5}{\left (5 \pi
        x\right)}\cos{\left(5 \pi x \right )}\]
which we can then set equal to $0$ and solve
\begin{minted}{python}
    # solve f'(x) = 0
    print(sympy.solveset(fp, x))
\end{minted}
to get $\frac{n}{5} - \frac{1}{10}$ after simplification:
% ditto
\[\left\{x \mid x \in \mathbb{C} \wedge \left(\left(- 4.938 x + 0.4938\right) \log{\left (2
            \right)} \sin{\left (5 \pi x \right )} + 30 \pi \cos{\left (5 \pi x \right )}\right)
    \sin^{5}{\left (5
        \pi x \right )} = 0 \right\} \setminus \left\{x \mid x \in \mathbb{C} \wedge 2^{2
            \left(1.11 x -
            0.11\right)^{2}} = 0 \right\}\]

It's also helpful to compare the results of your typical gradient optimization method against the
methods discussed later in this report.

\begin{minted}{python}
    # Convert the symbolic derivative to a numpy-compatable function for plotting.
    fp = sympy.lambdify([x], fp, 'numpy')

    # Minimizing -f(x) will maximize it.
    result = scipy.optimize.minimize(lambda x: -f(x), 0.19, bounds=[(0, 1)])
    print(result.x)
    result = scipy.optimize.minimize(lambda x: -f(x), 0.2, bounds=[(0, 1)])
    print(result.x)
    result = scipy.optimize.minimize(lambda x: -f(x), 0.21, bounds=[(0, 1)])
    print(result.x)
\end{minted}
\vspace{-1cm}
\begin{minted}{text}
    0.1
    0.2
    0.29953865
\end{minted}
\vspace{-1cm}

On either side of the minima at $x = 0.2$ the gradient optimizer\footnote{The optimization
    algorithm scipy chose for this function was the L-BFGS-B algorithm. I don't know anything about
    how well and under what conditions it performs, but I assume it works better than anything I
    could write by hand.} finds the next closest local maxima. This agrees with my intuitive
understanding of gradient methods; without some kind of correction, they too are susceptible to
finding local optima of oscillatory functions.

\subsubsection{Hill Climbing}

This hill climbing algorithm discussed in the book is listed in \autoref{alg:hill-climbing}.

\begin{algorithm}[h]
    % \begin{noindent}
    \begin{algorithmic}
        \Function{hill-climbing}{$f$}
            \State{Initialize $x$}
            \While{not done}\IComment{can be convergence or fixed iteration}
                \State{$x' = x + \text{perturbation}$}
                \If{$f(x') < f(x)$}\IComment{this minimizes $f(x)$}
                    \State{$x = x'$}
                \EndIf{}
            \EndWhile{}
            \State\Return{$x$}
        \EndFunction{}
    \end{algorithmic}
    % \end{noindent}
    \caption{The hill climbing algorithm}\label{alg:hill-climbing}
\end{algorithm}

This can easily be implemented in Python as

\begin{minted}{python}
    def hill_climbing(func, bounds, sigma, iters):
        """Minimize the given function using the Hill Climbing algorithm.

        :param func: The function to minimize.
        :param bounds: The lower and upper bounds on the feasible region.
        :param sigma: The standard deviation to use when perturbing the current guess.
        :param iters: The number of iterations to run the hill climbing algorithm for.
        :returns: The path of points visited from the initial guess to the final solution.
        """
        path = np.zeros(iters)
        x0 = np.random.uniform(*bounds)
        for i in range(iters):
            xp = perturb(x0, bounds, sigma)
            if func(xp) < func(x0):
                x0 = xp
            path[i] = x0
        return path
\end{minted}

with \mintinline{python}{perturb(x, bounds, sigma)} defined as

\begin{minted}{python}
    def perturb(x, bounds, sigma):
        """Perturb the given value by adding zero-mean white noise.

        :param x: The value to perturb.
        :param bounds: The lower and upper bounds on the feasible region.
        :param sigma: The standard deviation to use when adding white noise.
        """
        m, M = bounds
        xp = x + np.random.normal(scale=sigma)
        while xp >= M or xp <= m:
            xp = x + np.random.normal(scale=sigma)
        return xp
\end{minted}

Note that my implementation of \mintinline{python}{perturb(x)} will not perturb the given point
outside of the feasible region for the problem.

One of the easiest ways to improve the quality of the results from the hill climbing algorithm is
to run it multiple times and take the best result. This is the iterated hill climbing algorithm
from the book, listed in \autoref{alg:iterated-hill-climbing}.

\begin{algorithm}[h]
    % \begin{noindent}
    \begin{algorithmic}
        \Function{iterated-hill-climbing}{$f, n$}
            \State{$solutions = map\big(\Call{hill-climbing}{f}, \{1, \dots, n\}\big)$}\IComment{A trivially parallelizable operation}
            \State\Return{The best solution}
        \EndFunction{}
    \end{algorithmic}
    % \end{noindent}
    \caption{The iterated hill climbing algorithm}\label{alg:iterated-hill-climbing}
\end{algorithm}

This algorithm can easily (even with parallelization) be implemented in Python as follows
\begin{minted}[highlightlines={18}]{python}
import itertools
import multiprocessing
# I wonder why it's called dummy?
from multiprocessing.dummy import Pool as ThreadPool

def iterated_hill_climbing(func, bounds, sigma, inner_iters, iters):
    """Repeatedly climb the hill to find the less-local extremum.

    :param func: The function to minimize.
    :param bounds: The lower and upper bounds on the feasible region.
    :param sigma: The standard deviation to use when perturbing the current guess.
    :param inner_iters: The number of iterations to use for each run of the algorithm.
    :param iters: The number of times to run the algorithm.
    :returns: An array of solutions from each run, sorted by their fitness.
    """
    pool = ThreadPool(multiprocessing.cpu_count())
    # starmap consumes the given iterable in parallel until it is exhausted, collecting the results.
    results = pool.starmap(hill_climbing, itertools.repeat((func, bounds, sigma, inner_iters), times=iters))
    # Each result is a full path, not the optimal value
    optimums = [r[-1] for r in results]
    # Sort the optimums by their fitness.
    optimums.sort(key=func)
    return np.array(optimums)
\end{minted}
This implementation runs the desired number of iterations in parallel using as many threads as you
have processors before collecting all of the results and sorting the optimums by the objective
function.

\subsubsection{Simulated Annealing}

The simulated annealing algorithm from the book (listed in \autoref{alg:simulated-annealing}) is
far more interesting. Similar to the hill climbing algorithm, it too perturbs the current solution
in order to move around the feasible region. However, it not only accepts solutions that improve
the current solution's fitness, it accepts solutions that \textit{decrease} the current solution's
fitness with probability decreasing with the system temperature.

The simulated annealing algorithm accepts such solutions in an attempt to move out of local optima.
But in order to converge, and to a more reasonable value, it does so with a decreasing probability.

\begin{algorithm}[H]
    % \begin{noindent}
    \begin{algorithmic}
        \Function{simulated-annealing}{$f$}
            \State{Initialize $T$}
            \State{Initialize $x$}
            \While{not done}\IComment{prefer convergence over fixed iterations}
                \State{$x' = x + \text{perturbation}$ }
                \If{$\displaystyle rand(0, 1) < e^{\frac{\left(f(x) - f(x')\right)}{T}}$}
                    \State{$x = x'$}
                \EndIf{}
                \State{Update $T$}
            \EndWhile{}
            \State\Return{$x$}
        \EndFunction{}
    \end{algorithmic}
    % \end{noindent}
    \caption{The simulated annealing algorithm}\label{alg:simulated-annealing}
\end{algorithm}

\autoref{alg:simulated-annealing} too, isn't difficult to implement\footnote{Although it can be a
    bitch to tune.} in Python. The difficulty lies in the choice of the perturbation method and the
cooling schedule. I chose the same \mintinline{python}{perturb(x, bounds, sigma)} from above,
although as mentioned later, picking the proper standard deviation required significant trial and
error.

\begin{minted}{python}
    def simulated_annealing(func, bounds, sigma, temp, cooling_factor):
        """Use simulated annealing to optimize the given function.

        :param func: The function to minimize.
        :param bounds: The lower and upper bounds on the feasible region.
        :param sigma: The standard deviation to use when perturbing the current guess.
        :param temp: The initial temperature of the system.
        :param cooling_factor: How quickly the system should cool.
        :returns: An array of points *accepted* by the random condition.
        """
        # Pick a random starting point somewhere in the domain.
        x0 = np.random.uniform(*bounds)
        current = func(x0)
        path = []
        while temp > 0.001:
            xp = perturb(x0, bounds, sigma)
            potential = func(xp)
            if potential < current or np.random.random() < np.exp((current - potential) / temp):
                x0 = xp
                path.append(x0)
                current = potential
            temp *= 1 - cooling_factor

        return np.array(path)
\end{minted}

Note that my implementation does not return an array of points at each iteration of the algorithm.
It instead returns an array of points that the algorithm has accepted, whether the point improved
the current solution's fitness or not. Also, note that the number of iterations the simulated
annealing algorithm takes to converge depends on the initial temperature of the system and the
cooling schedule. Contrast this with the hill climbing algorithm, which terminates after some
predetermined number of steps.

Although not required, I implemented an iterated version of the simulated annealing algorithm,
echoing \autoref{alg:iterated-hill-climbing}.

\begin{algorithm}[H]
    % \begin{noindent}
    \begin{algorithmic}
        \Function{iterated-simulated-annealing}{$f, n$}
            \State{$solutions = map\big(\Call{simulated-annealing}{f}, \{1, \dots, n\}\big)$}\IComment{A trivially parallelizable operation}
            \State\Return{The best solution}
        \EndFunction{}
    \end{algorithmic}
    % \end{noindent}
    \caption{The iterated simulated annealing algorithm}\label{alg:iterated-simulated-annealing}
\end{algorithm}

The algorithms are essentially identical, as are their implementations.

\begin{minted}[highlightlines={14}]{python}
    def iterated_simulated_annealing(func, bounds, sigma, temp, cooling_factor, iters):
        """Repeatedly run simulated annealing to improve the quality of the results.

        :param func: The function to minimize.
        :param bounds: The lower and upper bounds on the feasible region.
        :param sigma: The standard deviation to use when perturbing the current guess.
        :param temp: The initial temperature of the system.
        :param cooling_factor: How quickly the system should cool.
        :param iters: The number of times to run the algorithm.
        :returns: An array of solutions from each run, sorted by their fitness.
        """
        pool = ThreadPool(multiprocessing.cpu_count())
        # starmap consumes the given iterable in parallel until it is exhausted, collecting the results.
        results = pool.starmap(simulated_annealing, itertools.repeat((func, bounds, sigma, temp, cooling_factor), times=iters))
        # Each result is a full path, not the optimal value
        optimums = [r[-1] for r in results]
        # Sort the optimums by their fitness.
        optimums.sort(key=func)
        return np.array(optimums)
\end{minted}

\subsection{Results}

\subsubsection{Hill Climbing}

Recall that the implementation of \autoref{alg:hill-climbing} picks a random starting point
somewhere inside the feasible region. There is also randomness in the perturbation of the current
solution as the algorithm progresses. This means one cannot run the algorithm once to get a sense
of how well it performs. Thus I ran the hill climbing algorithm a great many times\footnote{Three.
    I ran it three times.} and plotted the results, summarized in
\autoref{fig:prob1:hill-climbing-results}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{prob1/figures/prob1-hill-climbing-results.pdf}
    \caption{Results from successive runs of the hill climbing
        algorithm}\label{fig:prob1:hill-climbing-results}
\end{figure}

In the left column, there is the accepted solution graphed as a function of the iteration number.
In the right column, there is a plot of the accepted solutions overlayed on top of the objective
value.

Interestingly, the plot of the accepted solution as a function of the iteration number was often a
very ``stairsteppy'' function, with riser height of $0.1$. That is, the accepted solutions very
often seem to jump from peak to peak, without hitting the sides of the peaks. I believe this is a
figment of my imagination, and is a consequence of the combination of the hill climbing algorithm
only accepting values that improve the current solution and the nature of the function being
optimized --- with steep, narrow peaks.

Since the iterated hill climbing algorithm in \autoref{alg:iterated-hill-climbing} takes the best
solution out of $n$ runs of the algorithm, it's useful to plot the results of all $n$ runs on the
same plot. \autoref{fig:prob1:iterated-hill-climbing-solutions} shows the results of the iterated
hill climbing algorithm.

\todoinline{Mention the sensitivity to the initial values.}

\begin{figure}[h]
    \centering
    \includegraphics{prob1/figures/prob1-hill-climbing-solution.pdf}
    \caption{Solutions from multiple runs of the hill climbing
        algorithm}\label{fig:prob1:iterated-hill-climbing-solutions}
\end{figure}

Notice that every near\footnote{It's bad form to save a binary file in a Git repository, so I have
    to generate these plots every time I build this document from scratch. Since the algorithms are
    random, this results in different plots every time --- so the result that makes it into the
    final version may not be what I'm looking at now. There are fixes for this problem, but none
    that I care to implement.} every peak is a solution returned by at least one iteration of the
hill climbing algorithm. Also note that the solutions cluster around the peaks, rather than
converge on the actual peaks themselves. This is due to the steep nature of each peak; a small
change in $x$ results in a fairly large change in $f(x)$. This means that random perturbations of
$x$ are more likely to land below the current solution than above.

\subsubsection{Simulated Annealing}

After a considerable\footnote{As an aside, it's very frustrating to not know if you're being an
    idiot and implemented an algorithm incorrectly, or if you're being an idiot and picked
    unreasonable metaparameters.} amount of tuning, I was able to get simulated annealing to yield
better results than the hill climbing algorithm. The tunable metaparameters that I tweaked were the
standard deviation of the perturbation function, the initial temperature, and the cooling factor.

Picking an appropriate standard deviation is important because the scale of the domain is small
enough that noise following a standard normal distribution is \textit{too} noisy --- resulting in
convergence to a random peak \textit{despite exploring the entire damn feasible region}. Picking
the cooling factor is important because we need to give the algorithm enough time to get its
rebellious teenager phase out of the way before it starts settling down to a single value.

The initial temperature of the system is interesting. After finding good values for the
perturbation\footnote{Fun fact: I've had enough to drink that I first spelled this as
    ``preturnabtion'', which is bad enough that my spell-checking extension failed to detect as an
    error.} standard deviation and the cooling factor, I attempted to use temperatures of the same
order of magnitude as the example solution of the Travelling Salesman Problem. Such temperatures
\textit{did} yield correct solutions, but only after an extreme amount of exploration; it took
longer to display the plot than it did to generate the solutions due to the amount of data
involved.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{prob1/figures/prob1-simulated-annealing-results.pdf}
    \caption{Simulated annealing results after an inordinate amount of
        tuning}\label{fig:prob1:simulated-annealing-results}
\end{figure}

After figuring out appropriate values for the perturbation standard deviation and the cooling
schedule, I was able to observe that modifying the initial temperature did not seem to modify the
quality of the solution, or the value the iterated algorithm converged to, but \textit{did} impact
the rate of convergence. I attempted values ranging from $8000$ to $0.002$, all of which eventually
converged to the same value.

I eventually settled on an initial temperature of $0.01$ even though $0.002$ worked just as well,
because an iterative algorithm that converges after only $15$ iterations isn't very exciting.

\todoinline{Mention the sensitivity to the initial values.}

\begin{figure}[h]
    \centering
    \includegraphics{prob1/figures/prob1-simulated-annealing-solutions.pdf}
    \caption{Iterated simulated annealing results}\label{fig:prob1:simulated-annealing-solutions}
\end{figure}

Observe from \autoref{fig:prob1:simulated-annealing-solutions} that, with the proper
metaparameters, the simulated annealing algorithm has \textit{much} better results than the hill
climbing results summarized in \autoref{fig:prob1:iterated-hill-climbing-solutions}. Each of the
runs of the simulated annealing algorithm results in the same peak marked as the solution. Further,
note that the solutions \textit{seem to be the same point}, as opposed to the fairly loose clusters
of the hill climbing algorithm. This is a consequence of the convergence condition of the simulated
annealing algorithm, versus the fixed number of iterations of the hill climbing algorithm.

In summary, I have several observations.
\begin{itemize}
    \item The simulated annealing algorithm converges closer to the actual peaks of the objective
          function.
    \item Simulated annealing required a considerable amount of tuning.
    \item The tunable metaparameters that affected the correctness of the solution the most were
          the standard deviation of the perturbation, and the cooling factor. The parameter that
          affected speed of convergence the most was the initial temperature.
    \item The solutions from repeated runs of the simulated annealing algorithm clustered much more
          tightly than the solutions from repeated runs of the hill climbing algorithm.
    \item When considering the standard deviation of the perturbation, you want enough noise to
          explore the feasible region, yet not so cuh that you jump all over the place.
    \item Temperatures ranging from $8000$ to $0.002$ (because 0.001 is the convergence criterion)
          worked well, but smaller temperatures converged much faster.
\end{itemize}

\todoinline{Mention the sensitivity to the initial values.}

\section{The TSP with Mutation Only}\label{prob:2}

\subsection{Statement}
In lecture we addressed the Traveling Salesman Problem using Simulated Annealing. To speed up
convergence and increase the odds of finding the global extremal, it makes sense to try an
evolutionary algorithm. The mutation operator can be adapted from the SA algorithm. Skip
recombination in this problem. Write an evolutionary algorithm to solve the TSP as generated in the
sample program. Compare deterministic and stochastic selection operators.

\subsection{Method}
There are a number of decisions to make when implementing the standard evolutionary algorithm
listed in \autoref{alg:standard-ea}. There are a great many ways to perform each of the steps
(Initialization, Recombination, Mutation, and Selection, as well as the stopping criterion), and
picking the \textit{right} method constitutes the difficulty of the problem.

\begin{algorithm}[h]
    % \begin{noindent}
    \begin{algorithmic}
        \Function{standard-ea}{}
            \State{Initialize $population$}
            \While{not done}
                \State{Recombine $population$}
                \State{Mutate $population$}
                \State{Select $population$}
            \EndWhile{}
            \State\Return{$population$}
        \EndFunction{}
    \end{algorithmic}
    % \end{noindent}
    \caption{The standard evolutionary algorithm}\label{alg:standard-ea}
\end{algorithm}

In this problem, there will be no recombination of the population, only mutation and selection.
Recombination will be done in \autoref{prob:3}, however, so it is useful to keep recombination in
mind when implementing the solution for this problem.

\subsubsection{Solution Representation}
Of particular importance is the choice of solution representation. The representations for a
problem make an extreme effect on the performance, ease, and usability of a solution. In many cases
even, your choice of data structure makes the proper algorithm for solving the problem obvious,
while an improper choice makes implementing the solution tedious, nonintuitive, and ill-performant.

For this problem, the most obvious representation for a solution is an ordered path
\[\lbrack C_1, C_2, \dots C_n \rbrack \]
of cities. There are other representations, however. As a reference,~\cite{tsp_ea} enumerates a
great deal of variations for the representations of solutions, mutation operators, and
recombination operators. Briefly, the solution representation options that~\cite{tsp_ea} lists are
listed\footnote{I'm running out of nested \LaTeX{} subsections!} below.

\paragraph{Bitstring}
The bitstring representation is the most common solution representation in traditional evolutionary
and genetic algorithm literature. In this representation, with a problem with $n$ cities, a
particular city is encoded as a bitstring of length $\ceil*{\log_2 n}$, and a path of cities (a
solution) as a bitstring of $n\ceil*{\log_2 n}$ concatenated cities.

However, this representation provides some difficulty when implementing \autoref{alg:standard-ea}.
Namely, the mutation and recombination operators must be more sophisticated than simple bitwise
operations because some $\ceil*{\log_2 n}$ long bitstrings might not represent a valid city, and
some $n\ceil*{\log_2 n}$ long bitstrings might not represent a valid solution to the problem. There
are some solutions to these problems, which~\cite{tsp_ea} mentions, but they are more tedious than
I would prefer.

\paragraph{Matrix}
The paper~\cite{tsp_ea} lists two different binary matrix representations of a solution.
\begin{enumerate}
    \item Represent an individual as a matrix $M = \{m_{ij}\}$ where $m_{ij} = 1$ if and only if
          the city $i$ is visited before the city $j$ in the represented tour. The paper lists the
          following properties that hold for a valid tour represented in this manner.
          \begin{enumerate}
              \item $\displaystyle \sum_i^n \sum_j^n m_{ij} = \frac{n(n - 1)}{2}$
              \item $\displaystyle m_{ii} = 0$
              \item $\displaystyle (m_{ij} = 1) \wedge (m_{jk} = 1) \Rightarrow m_{ik} = 1$
          \end{enumerate}
    \item Represent an individual as a matrix $M = \{m_{ij}\}$ where $m_{ij} = 1$ if and only if
          city $j$ is visited immediately after city $i$. That is, a valid tour is represented by a
          matrix with exactly one $1$ in every row and in every column.
\end{enumerate}

\paragraph{Adjacency List}
An uncommon representation is that of an adjacency list, where city $j$ occurs in the $i$th
position of the list if and only if the tour leads from city $i$ to city $j$. However, the proposed
mutation and recombination operators good subpaths in parents, so this representation is not widely
used.

\paragraph{Ordered Path}
As mentioned above, this is the most obvious representation, and the one~\cite{tsp_ea} exclusively
deals with. Here, a path is represented as an ordered list
\[C_1, C_2, \dots C_n \]
where $C_i$ occurs at the $j$th position if it is the $j$th city to be visited.

This representation yields itself nicely to the fitness evaluation of individuals in your
population, and is useful as the final representation of a returned solution. However, the
classical mutation and recombination operators typically performed on bitstrings will not work on
this representation because they can (will) result in invalid paths.

This will be the representation I use, and is thus the only representation I attempt to define
mutation and (in \autoref{prob:3}) recombination operators on. In this representation, we can
generate a random individual by

\begin{minted}{python}
    def generate_individual(n, scale=100):
        """Generates an individual solution of the given size.

        Pick random coordinates in the `scale`x`scale` grid uniformly.

        :param n: The size of the individual to generate.
        :param scale: How much to scale the individual's coordinates by.
        """
        # Scale up the uniform values from [0, 1].
        return np.random.rand(n, 2) * scale
\end{minted}

and a random population of a given size by

\begin{minted}{python}
    def generate_population(size, n, scale=100):
        """Generate a population of individuals with the given size.

        Pick random coordinates in the `scale`x`scale` grid uniformly.

        :param size: The number of individuals to generate.
        :param n: The size of each individual.
        :param scale: How much to scale the individual's coordinates by.
        """
        # Scale up the uniform values from [0, 1].
        return np.random.rand(size, n, 2) * scale
\end{minted}

The \mintinline{python}{generate_individual()} function generates a single individual as shown in
\autoref{fig:prob2:random-individual}.

\begin{figure}[h]
    \centering
    \includegraphics{prob2/figures/prob2-random-individual.pdf}
    \caption{A randomly generated individual}\label{fig:prob2:random-individual}
\end{figure}

Note that each city is an $(x, y)$ pair so that we can more easily compute the distance between
them without explicitly saving that information in a graph datastructure.

\subsubsection{Fitness}
With this representation as an individual as an ordered list of ordered pairs, an individual's
solution is given as the sum of the pairwise distances between cities. To implement this, it is
helpful to use the
\href{https://docs.python.org/3/library/itertools.html#itertools-recipes}{itertools pairwise
    recipe} for iterating over a sequence in pairs.
\begin{minted}{python}
    import itertools

    def pairwise(iterable):
        """Iterate over the given iterable in pairs.

        pairwise([1, 2, 3, 4]) -> (1, 2), (2, 3), (3, 4)
        """
        a, b = itertools.tee(iterable)
        # Advance b one step
        next(b, None)
        return zip(a, b)
\end{minted}
Then the fitness function for an individual can be written succinctly as
\begin{minted}{python}
    def fitness(individual):
        """Evaluate the fitness of the given individual.

        Compute the Euclidean distance between every pair of cities in the individual
        and add them together.

        :param individual: An array of cities, where each city is an (x, y) pair.
        :type individual: np.ndarray with shape (n, 2)
        :return: The fitness of the individual.
        """
        return sum(np.linalg.norm(c1 - c2) for c1, c2 in pairwise(individual))
\end{minted}

\subsubsection{Mutation Methods}
There are a number of possible mutation operators that will work on the chosen solution
representation.

\paragraph{Swap Mutation} This mutation randomly swaps two cities in the tour.

\paragraph{Insertion Mutation} This mutation is similar to a swap. It picks a random city, removes
it from the list, and inserts it as some randomly selected position.
\paragraph{Displacement Mutation} This is a natural extension of the insertion mutation. Rather
than removing and inserting a single city, remove and insert an entire subpath.

\paragraph{Scramble Mutation} This mutation is not used much, because it can destroy good genetic
information in a parent. It picks a random subpath in a solution and shuffles it.

\paragraph{Inversion Mutation} This is the mutation used in the simulated annealing example shown
in class. It picks a random subpath in a solution, and reverses it. This is a useful operator
because it breaks only two links between cities, and leaves the rest intact. All of the other
mutations can be implemented as a sequence of inversions. A proof of this remark is given in
\autoref{app:proof}. This mutation can also be extended to act like the displacement mutation where
a random subpath is removed, inverted, and reinserted somewhere else in the tour.

\todoinline{Implement (at least) the inversion mutation. Use a keyword argument to pick
    mutation.}

\subsubsection{Selection Methods}
In my implementation, I will deal with two different selection methods: stochastic and
deterministic. The stochastic selection will rank the population according to their fitness and
sample the sorted population by some probability distribution where the probability of being chosen
is directly proportional to an individual's fitness. The deterministic method will rank the
population by their fitness and select the top $n$ to survive.

\todoinline{Implement both selection methods. Use keyword argument to pick deterministic or
    stochastic.}

\subsection{Results}
\todoinline{
    Implement EA with different components.

    Compare solutions of different mutation and selection operators.

    Compare deterministic and stochastic selection operators.
}

\section{The TSP with Mutation and Recombination}\label{prob:3}

\subsection{Statement}
In \autoref{prob:2} we implemented EA code to solve the Traveling Salesman Problem. In this
problem, implement recombination (crossover) in your EA. For this problem you will need to use an
encoding that prevents crossover that creates an invalid candidate. As before, compare
deterministic and stochastic selection operators.

\subsection{Method}
\todoinline{
    List recombination methods from~\cite{tsp_ea}.

    Implement crossover.
}

\subsection{Results}
\todoinline{
    Compare selection operators.

    Compare recombination operators.
}

\appendix\appendixpage{}\addappheadtotoc{}
\section{Inversion Proof}\label{app:proof}

Since the best proofs are constructive, I thought I would get meta and programmatically construct a
proof that swaps, insertions, displacements, and shuffles can be implemented as a sequence of
inversions.

\inputminted{python}{proof.py}

Unfortunately, I cannot claim to be the original author of such an excellent class of program. The
inspiration came from a fellow sufferer of topology,
\href{https://www.reddit.com/user/kwprules}{u/kwprules}, a year previous to my own suffering. The
illuminated discussion on their proof technique may be found
\href{https://www.reddit.com/r/math/comments/7gqhlc/what\_to\_say\_instead\_of\_trivially/}{here}.

\bibliographystyle{ieeetr}
\bibliography{../homework}{}

\end{document}
