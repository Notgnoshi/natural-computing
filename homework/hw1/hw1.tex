\documentclass{article}
\input{../homework.sty}

\title{Homework 1}
\author{Austin Gill}

\begin{document}
\maketitle
\begingroup
\hypersetup{linkcolor=black}
\tableofcontents
% TODO: Remove this.
\listoftodos
\endgroup
\newpage

The Jupyter notebooks containing the work for each of the following problems, as well as the
\LaTeX{} source code for this document can be found at
\url{https://github.com/Notgnoshi/natural-computing/tree/master/homework/hw1}

\section{Function Optimization via Simulated Annealing}\label{prob:1}

\subsection{Statement}
Compare the effectiveness of the text's iterated hill climbing and simulated annealing algorithms
to find the max of
\[ f(x) = 2^{-2{\left(\frac{(x - 0.1)}{0.9}\right)}^2}{\big(\sin(5\pi x)\big)}^6\]
with $x\in [0,1]$. Use a real valued representation. Include a plot of the function with the
location of the max and a plot of the estimate as a function of the iteration number. How sensitive
are the algorithms to initial values?

\subsection{Method}

\subsubsection{Problem Setup and Gradient Methods}

For completeness and curiosity, I chose to attack the problem from a symbolic perspective first.
This gives an opportunity to get comfortable with the problem, and get my workflow setup correctly
because I've recently reinstalled Ubuntu and subsequently all of my commonly used Python libraries.

The python \mintinline{python}{sympy} library provides decent symbolic computation capabilities
that I have had good luck with in the past. The only real surprising implementation detail is in
the definition of the objective function.

\begin{minted}[highlightlines={7-8}]{python}
def f(x):
    """The objective function to evaluate.

    This function returns a sympy symbolic function, float, or np.ndarray
    depending on the type of the input.
    """
    # Use symbolic sine, pi if necessary.
    sin, pi = (sympy.sin, sympy.pi) if isinstance(x, sympy.Symbol) else (np.sin, np.pi)

    return 2 ** (-2 * ((x - 0.1) / 0.9) ** 2) * sin(5 * pi * x) ** 6
\end{minted}

If the given type is a symbolic variable, return a symbolic representation of the function rather
than the default numerical computation. Then this function can be given individual float values,
numpy arrays, and sympy symbols.

\begin{figure}[h]
    \centering
    % Make sure to run the notebook before this will compile.
    \includegraphics{prob1/figures/prob1-function.pdf}
    \caption{The objective function $f(x)$}\label{fig:prob1:function}
\end{figure}

The very first thing to do when dealing with optimizing a function is to plot anything you can get
your hands on, including the neighbor's cat. The objective function and its derivative are shown in
\autoref{fig:prob1:function} and \autoref{fig:prob1:derivative} respectively.

\begin{figure}[h]
    \centering
    \includegraphics{prob1/figures/prob1-derivative.pdf}
    \caption{The objective function's derivative}\label{fig:prob1:derivative}
\end{figure}

We can immediately pick out the local and global extrema by visual inspection. The maxima
periodically occur at $\frac{n}{5} - \frac{1}{10}$ with the global maximum occuring at
$\frac{1}{10}$. The minima periodically occur at $\frac{n}{5}$ with value $0$.

We can produce a symbolic version of the objective function easily with
\begin{minted}{python}
    x = sympy.Symbol('x')
    print(f(x))
\end{minted}
which displays\footnote{When coerced with \mintinline{python}{sympy.init_printing()} in a Jupyter
    notebook.}
\[2^{- 2 {\left(1.11 x - 0.11\right)}^{2}} \sin^{6}{\left (5 \pi x
        \right)}\]
Then we compute the atrocious derivative symbolically.
\begin{minted}{python}
    fp = sympy.diff(f(x), x)
    print(fp)
\end{minted}
which displays the trivial derivative\footnote{Coincidentally, the programmatically generated, and
    subsequently auto-formatted \LaTeX{} code for these functions is, I think, the ugliest thing
    I've ever seen.}
% This garbage is what sympy + my autoformatter gives. I'm not fixing it.
\[2^{- 2 \left(1.11 x - 0.11\right)^{2}} \left(- 4.938 x +
    0.4938\right) \log{\left (2 \right )} \sin^{6}{\left (5 \pi x \right )} + 30 \cdot
    2^{-2
            \left(1.11 x - 0.11\right)^{2}} \pi \sin^{5}{\left (5 \pi
        x\right)}\cos{\left(5 \pi x \right )}\]
which we can then set equal to $0$ and solve
\begin{minted}{python}
    # solve f'(x) = 0
    print(sympy.solveset(fp, x))
\end{minted}
to get $\frac{n}{5} - \frac{1}{10}$ after simplification:
% ditto
\[\left\{x \mid x \in \mathbb{C} \wedge \left(\left(- 4.938 x + 0.4938\right) \log{\left (2
            \right)} \sin{\left (5 \pi x \right )} + 30 \pi \cos{\left (5 \pi x \right )}\right)
    \sin^{5}{\left (5
        \pi x \right )} = 0 \right\} \setminus \left\{x \mid x \in \mathbb{C} \wedge 2^{2
            \left(1.11 x -
            0.11\right)^{2}} = 0 \right\}\]

It's also helpful to compare the results of your typical gradient optimization method against the
methods discussed later in this report.

\begin{minted}{python}
    # Convert the symbolic derivative to a numpy-compatable function for plotting.
    fp = sympy.lambdify([x], fp, 'numpy')

    # Minimizing -f(x) will maximize it.
    result = scipy.optimize.minimize(lambda x: -f(x), 0.19, bounds=[(0, 1)])
    print(result.x)
    result = scipy.optimize.minimize(lambda x: -f(x), 0.2, bounds=[(0, 1)])
    print(result.x)
    result = scipy.optimize.minimize(lambda x: -f(x), 0.21, bounds=[(0, 1)])
    print(result.x)
\end{minted}
\vspace{-1cm}
\begin{minted}{text}
    0.1
    0.2
    0.29953865
\end{minted}
\vspace{-1cm}

On either side of the minima at $x = 0.2$ the gradient optimizer\footnote{The optimization
    algorithm scipy chose for this function was the L-BFGS-B algorithm. I don't know anything about
    how well and under what conditions it performs, but I assume it works better than anything I
    could write by hand.} finds the next closest local maxima. This agrees with my intuitive
understanding of gradient methods; without some kind of correction, they too are susceptible to
finding local optima of oscillatory functions.

\subsubsection{Hill Climbing}

This hill climbing algorithm discussed in the book is listed in \autoref{alg:hill-climbing}.

\begin{algorithm}[h]
    % \begin{noindent}
    \begin{algorithmic}
        \Function{hill-climbing}{$f$}
            \State{Initialize $x$}
            \While{not done}\IComment{can be convergence or fixed iteration}
                \State{$x' = x + \text{perturbation}$}
                \If{$f(x') < f(x)$}\IComment{this minimizes $f(x)$}
                    \State{$x = x'$}
                \EndIf{}
            \EndWhile{}
            \State\Return{$x$}
        \EndFunction{}
    \end{algorithmic}
    % \end{noindent}
    \caption{The hill climbing algorithm}\label{alg:hill-climbing}
\end{algorithm}

This can easily be implemented in Python as

\begin{minted}{python}
    def hill_climbing(func, bounds, sigma, iters):
        """Minimize the given function using the Hill Climbing algorithm.

        :param func: The function to minimize.
        :param bounds: The lower and upper bounds on the feasible region.
        :param sigma: The standard deviation to use when perturbing the current guess.
        :param iters: The number of iterations to run the hill climbing algorithm for.
        :returns: The path of points visited from the initial guess to the final solution.
        """
        path = np.zeros(iters)
        x0 = np.random.uniform(*bounds)
        for i in range(iters):
            xp = perturb(x0, bounds, sigma)
            if func(xp) < func(x0):
                x0 = xp
            path[i] = x0
        return path
\end{minted}

with \mintinline{python}{perturb(x, bounds, sigma)} defined as

\begin{minted}{python}
    def perturb(x, bounds, sigma):
        """Perturb the given value by adding zero-mean white noise.

        :param x: The value to perturb.
        :param bounds: The lower and upper bounds on the feasible region.
        :param sigma: The standard deviation to use when adding white noise.
        """
        m, M = bounds
        xp = x + np.random.normal(scale=sigma)
        while xp >= M or xp <= m:
            xp = x + np.random.normal(scale=sigma)
        return xp
\end{minted}

Note that my implementation of \mintinline{python}{perturb(x)} will not perturb the given point
outside of the feasible region for the problem.

One of the easiest ways to improve the quality of the results from the hill climbing algorithm is
to run it multiple times and take the best result. This is the iterated hill climbing algorithm
from the book, listed in \autoref{alg:iterated-hill-climbing}.

\begin{algorithm}[h]
    % \begin{noindent}
    \begin{algorithmic}
        \Function{iterated-hill-climbing}{$f, n$}
            \State{$solutions = map\big(\Call{hill-climbing}{f}, \{1, \dots, n\}\big)$}\IComment{A trivially parallelizable operation}
            \State\Return{The best solution}
        \EndFunction{}
    \end{algorithmic}
    % \end{noindent}
    \caption{The iterated hill climbing algorithm}\label{alg:iterated-hill-climbing}
\end{algorithm}

This algorithm can easily (even with parallelization) be implemented in Python as follows
\begin{minted}[highlightlines={18}]{python}
import itertools
import multiprocessing
# I wonder why it's called dummy?
from multiprocessing.dummy import Pool as ThreadPool

def iterated_hill_climbing(func, bounds, sigma, inner_iters, iters):
    """Repeatedly climb the hill to find the less-local extremum.

    :param func: The function to minimize.
    :param bounds: The lower and upper bounds on the feasible region.
    :param sigma: The standard deviation to use when perturbing the current guess.
    :param inner_iters: The number of iterations to use for each run of the algorithm.
    :param iters: The number of times to run the algorithm.
    :returns: An array of solutions from each run, sorted by their fitness.
    """
    pool = ThreadPool(multiprocessing.cpu_count())
    # starmap consumes the given iterable in parallel until it is exhausted, collecting the results.
    results = pool.starmap(hill_climbing, itertools.repeat((func, bounds, sigma, inner_iters), times=iters))
    # Each result is a full path, not the optimal value
    optimums = [r[-1] for r in results]
    # Sort the optimums by their fitness.
    optimums.sort(key=func)
    return np.array(optimums)
\end{minted}
This implementation runs the desired number of iterations in parallel using as many threads as you
have processors before collecting all of the results and sorting the optimums by the objective
function.

\subsubsection{Simulated Annealing}

The simulated annealing algorithm from the book (listed in \autoref{alg:simulated-annealing}) is
far more interesting. Similar to the hill climbing algorithm, it too perturbs the current solution
in order to move around the feasible region. However, it not only accepts solutions that improve
the current solution's fitness, it accepts solutions that \textit{decrease} the current solution's
fitness with probability decreasing with the system temperature.

The simulated annealing algorithm accepts such solutions in an attempt to move out of local optima.
But in order to converge, and to a more reasonable value, it does so with a decreasing probability.

\begin{algorithm}[H]
    % \begin{noindent}
    \begin{algorithmic}
        \Function{simulated-annealing}{$f$}
            \State{Initialize $T$}
            \State{Initialize $x$}
            \While{not done}\IComment{prefer convergence over fixed iterations}
                \State{$x' = x + \text{perturbation}$ }
                \If{$\displaystyle rand(0, 1) < e^{\frac{\left(f(x) - f(x')\right)}{T}}$}
                    \State{$x = x'$}
                \EndIf{}
                \State{Update $T$}
            \EndWhile{}
            \State\Return{$x$}
        \EndFunction{}
    \end{algorithmic}
    % \end{noindent}
    \caption{The simulated annealing algorithm}\label{alg:simulated-annealing}
\end{algorithm}

\autoref{alg:simulated-annealing} too, isn't difficult to implement\footnote{Although it can be a
    bitch to tune.} in Python. The difficulty lies in the choice of the perturbation method and the
cooling schedule. I chose the same \mintinline{python}{perturb(x, bounds, sigma)} from above,
although as mentioned later, picking the proper standard deviation required significant trial and
error.

\begin{minted}{python}
    def simulated_annealing(func, bounds, sigma, temp, cooling_factor):
        """Use simulated annealing to optimize the given function.

        :param func: The function to minimize.
        :param bounds: The lower and upper bounds on the feasible region.
        :param sigma: The standard deviation to use when perturbing the current guess.
        :param temp: The initial temperature of the system.
        :param cooling_factor: How quickly the system should cool.
        :returns: An array of points *accepted* by the random condition.
        """
        # Pick a random starting point somewhere in the domain.
        x0 = np.random.uniform(*bounds)
        current = func(x0)
        path = []
        while temp > 0.001:
            xp = perturb(x0, bounds, sigma)
            potential = func(xp)
            if potential < current or np.random.random() < np.exp((current - potential) / temp):
                x0 = xp
                path.append(x0)
                current = potential
            temp *= 1 - cooling_factor

        return np.array(path)
\end{minted}

Note that my implementation does not return an array of points at each iteration of the algorithm.
It instead returns an array of points that the algorithm has accepted, whether the point improved
the current solution's fitness or not. Also, note that the number of iterations the simulated
annealing algorithm takes to converge depends on the initial temperature of the system and the
cooling schedule. Contrast this with the hill climbing algorithm, which terminates after some
predetermined number of steps.

Although not required, I implemented an iterated version of the simulated annealing algorithm,
echoing \autoref{alg:iterated-hill-climbing}.

\begin{algorithm}[H]
    % \begin{noindent}
    \begin{algorithmic}
        \Function{iterated-simulated-annealing}{$f, n$}
            \State{$solutions = map\big(\Call{simulated-annealing}{f}, \{1, \dots, n\}\big)$}\IComment{A trivially parallelizable operation}
            \State\Return{The best solution}
        \EndFunction{}
    \end{algorithmic}
    % \end{noindent}
    \caption{The iterated simulated annealing algorithm}\label{alg:iterated-simulated-annealing}
\end{algorithm}

The algorithms are essentially identical, as are their implementations.

\begin{minted}[highlightlines={14}]{python}
    def iterated_simulated_annealing(func, bounds, sigma, temp, cooling_factor, iters):
        """Repeatedly run simulated annealing to improve the quality of the results.

        :param func: The function to minimize.
        :param bounds: The lower and upper bounds on the feasible region.
        :param sigma: The standard deviation to use when perturbing the current guess.
        :param temp: The initial temperature of the system.
        :param cooling_factor: How quickly the system should cool.
        :param iters: The number of times to run the algorithm.
        :returns: An array of solutions from each run, sorted by their fitness.
        """
        pool = ThreadPool(multiprocessing.cpu_count())
        # starmap consumes the given iterable in parallel until it is exhausted, collecting the results.
        results = pool.starmap(simulated_annealing, itertools.repeat((func, bounds, sigma, temp, cooling_factor), times=iters))
        # Each result is a full path, not the optimal value
        optimums = [r[-1] for r in results]
        # Sort the optimums by their fitness.
        optimums.sort(key=func)
        return np.array(optimums)
\end{minted}

\subsection{Results}

\subsubsection{Hill Climbing}

Recall that the implementation of \autoref{alg:hill-climbing} picks a random starting point
somewhere inside the feasible region. There is also randomness in the perturbation of the current
solution as the algorithm progresses. This means one cannot run the algorithm once to get a sense
of how well it performs. Thus I ran the hill climbing algorithm a great many times\footnote{Three.
    I ran it three times.} and plotted the results, summarized in
\autoref{fig:prob1:hill-climbing-results}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{prob1/figures/prob1-hill-climbing-results.pdf}
    \caption{Results from successive runs of the hill climbing
        algorithm}\label{fig:prob1:hill-climbing-results}
\end{figure}

In the left column, there is the accepted solution graphed as a function of the iteration number.
In the right column, there is a plot of the accepted solutions overlayed on top of the objective
value.

Interestingly, the plot of the accepted solution as a function of the iteration number was often a
very ``stairsteppy'' function, with riser height of $0.1$. That is, the accepted solutions very
often seem to jump from peak to peak, without hitting the sides of the peaks. I believe this is a
figment of my imagination, and is a consequence of the combination of the hill climbing algorithm
only accepting values that improve the current solution and the nature of the function being
optimized --- with steep, narrow peaks.

Since the iterated hill climbing algorithm in \autoref{alg:iterated-hill-climbing} takes the best
solution out of $n$ runs of the algorithm, it's useful to plot the results of all $n$ runs on the
same plot. \autoref{fig:prob1:iterated-hill-climbing-solutions} shows the results of the iterated
hill climbing algorithm.

\todoinline{Mention the sensitivity to the initial values.}

\begin{figure}[h]
    \centering
    \includegraphics{prob1/figures/prob1-hill-climbing-solution.pdf}
    \caption{Solutions from multiple runs of the hill climbing
        algorithm}\label{fig:prob1:iterated-hill-climbing-solutions}
\end{figure}

Notice that every near\footnote{It's bad form to save a binary file in a Git repository, so I have
    to generate these plots every time I build this document from scratch. Since the algorithms are
    random, this results in different plots every time --- so the result that makes it into the
    final version may not be what I'm looking at now. There are fixes for this problem, but none
    that I care to implement.} every peak is a solution returned by at least one iteration of the
hill climbing algorithm. Also note that the solutions cluster around the peaks, rather than
converge on the actual peaks themselves. This is due to the steep nature of each peak; a small
change in $x$ results in a fairly large change in $f(x)$. This means that random perturbations of
$x$ are more likely to land below the current solution than above.

\subsubsection{Simulated Annealing}

After a considerable\footnote{As an aside, it's very frustrating to not know if you're being an
    idiot and implemented an algorithm incorrectly, or if you're being an idiot and picked
    unreasonable metaparameters.} amount of tuning, I was able to get simulated annealing to yield
better results than the hill climbing algorithm. The tunable metaparameters that I tweaked were the
standard deviation of the perturbation function, the initial temperature, and the cooling factor.

Picking an appropriate standard deviation is important because the scale of the domain is small
enough that noise following a standard normal distribution is \textit{too} noisy --- resulting in
convergence to a random peak \textit{despite exploring the entire damn feasible region}. Picking
the cooling factor is important because we need to give the algorithm enough time to get its
rebellious teenager phase out of the way before it starts settling down to a single value.

The initial temperature of the system is interesting. After finding good values for the
perturbation\footnote{Fun fact: I've had enough to drink that I first spelled this as
    ``preturnabtion'', which is bad enough that my spell-checking extension failed to detect as an
    error.} standard deviation and the cooling factor, I attempted to use temperatures of the same
order of magnitude as the example solution of the Travelling Salesman Problem. Such temperatures
\textit{did} yield correct solutions, but only after an extreme amount of exploration; it took
longer to display the plot than it did to generate the solutions due to the amount of data
involved.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{prob1/figures/prob1-simulated-annealing-results.pdf}
    \caption{Simulated annealing results after an inordinate amount of
        tuning}\label{fig:prob1:simulated-annealing-results}
\end{figure}

After figuring out appropriate values for the perturbation standard deviation and the cooling
schedule, I was able to observe that modifying the initial temperature did not seem to modify the
quality of the solution, or the value the iterated algorithm converged to, but \textit{did} impact
the rate of convergence. I attempted values ranging from $8000$ to $0.002$, all of which eventually
converged to the same value.

I eventually settled on an initial temperature of $0.01$ even though $0.002$ worked just as well,
because an iterative algorithm that converges after only $15$ iterations isn't very exciting.

\todoinline{Mention the sensitivity to the initial values.}

\begin{figure}[h]
    \centering
    \includegraphics{prob1/figures/prob1-simulated-annealing-solutions.pdf}
    \caption{Iterated simulated annealing results}\label{fig:prob1:simulated-annealing-solutions}
\end{figure}

Observe from \autoref{fig:prob1:simulated-annealing-solutions} that, with the proper
metaparameters, the simulated annealing algorithm has \textit{much} better results than the hill
climbing results summarized in \autoref{fig:prob1:iterated-hill-climbing-solutions}. Each of the
runs of the simulated annealing algorithm results in the same peak marked as the solution. Further,
note that the solutions \textit{seem to be the same point}, as opposed to the fairly loose clusters
of the hill climbing algorithm. This is a consequence of the convergence condition of the simulated
annealing algorithm, versus the fixed number of iterations of the hill climbing algorithm.

In summary, I have several observations.
\begin{itemize}
    \item The simulated annealing algorithm converges closer to the actual peaks of the objective
          function.
    \item Simulated annealing required a considerable amount of tuning.
    \item The tunable metaparameters that affected the correctness of the solution the most were
          the standard deviation of the perturbation, and the cooling factor. The parameter that
          affected speed of convergence the most was the initial temperature.
    \item The solutions from repeated runs of the simulated annealing algorithm clustered much more
          tightly than the solutions from repeated runs of the hill climbing algorithm.
    \item When considering the standard deviation of the perturbation, you want enough noise to
          explore the feasible region, yet not so cuh that you jump all over the place.
    \item Temperatures ranging from $8000$ to $0.002$ (because 0.001 is the convergence criterion)
          worked well, but smaller temperatures converged much faster.
\end{itemize}

\todoinline{Mention the sensitivity to the initial values.}

\section{The TSP with Mutation Only}\label{prob:2}

\subsection{Statement}
In lecture we addressed the Traveling Salesman Problem using Simulated Annealing. To speed up
convergence and increase the odds of finding the global extremal, it makes sense to try an
evolutionary algorithm. The mutation operator can be adapted from the SA algorithm. Skip
recombination in this problem. Write an evolutionary algorithm to solve the TSP as generated in the
sample program. Compare deterministic and stochastic selection operators.

\subsection{Method}
There are a number of decisions to make when implementing the standard evolutionary algorithm
listed in \autoref{alg:standard-ea}. There are a great many ways to perform each of the steps
(Initialization, Recombination, Mutation, and Selection, as well as the stopping criterion), and
picking the \textit{right} method constitutes the difficulty of the problem.

\begin{algorithm}[h]
    % \begin{noindent}
    \begin{algorithmic}
        \Function{standard-ea}{}
            \State{Initialize $population$}
            \While{not done}
                \State{Recombine $population$}
                \State{Mutate $population$}
                \State{Select $population$}
            \EndWhile{}
            \State\Return{$population$}
        \EndFunction{}
    \end{algorithmic}
    % \end{noindent}
    \caption{The standard evolutionary algorithm}\label{alg:standard-ea}
\end{algorithm}

In this problem, there will be no recombination of the population, only mutation and selection.
Recombination will be done in \autoref{prob:3}, however, so it is useful to keep recombination in
mind when implementing the solution for this problem.

\subsubsection{Solution Representation}
Of particular importance is the choice of solution representation. The representations for a
problem make an extreme effect on the performance, ease, and usability of a solution. In many cases
even, your choice of data structure makes the proper algorithm for solving the problem obvious,
while an improper choice makes implementing the solution tedious, nonintuitive, and ill-performant.

For this problem, the most obvious representation for a solution is an ordered path
\[\lbrack C_1, C_2, \dots C_n \rbrack \]
of cities. There are other representations, however. As a reference,~\cite{tsp_ea} enumerates a
great deal of variations for the representations of solutions, mutation operators, and
recombination operators. Briefly, the solution representation options that~\cite{tsp_ea} lists are
listed\footnote{I'm running out of nested \LaTeX{} subsections!} below.

\paragraph{Bitstring}
The bitstring representation is the most common solution representation in traditional evolutionary
and genetic algorithm literature. In this representation, with a problem with $n$ cities, a
particular city is encoded as a bitstring of length $\ceil*{\log_2 n}$, and a path of cities (a
solution) as a bitstring of $n\ceil*{\log_2 n}$ concatenated cities.

However, this representation provides some difficulty when implementing \autoref{alg:standard-ea}.
Namely, the mutation and recombination operators must be more sophisticated than simple bitwise
operations because some $\ceil*{\log_2 n}$ long bitstrings might not represent a valid city, and
some $n\ceil*{\log_2 n}$ long bitstrings might not represent a valid solution to the problem. There
are some solutions to these problems, which~\cite{tsp_ea} mentions, but they are more tedious than
I would prefer.

\paragraph{Matrix}
The paper~\cite{tsp_ea} lists two different binary matrix representations of a solution.
\begin{enumerate}
    \item Represent an individual as a matrix $M = \{m_{ij}\}$ where $m_{ij} = 1$ if and only if
          the city $i$ is visited before the city $j$ in the represented tour. The paper lists the
          following properties that hold for a valid tour represented in this manner.
          \begin{enumerate}
              \item $\displaystyle \sum_i^n \sum_j^n m_{ij} = \frac{n(n - 1)}{2}$
              \item $\displaystyle m_{ii} = 0$
              \item $\displaystyle (m_{ij} = 1) \wedge (m_{jk} = 1) \Rightarrow m_{ik} = 1$
          \end{enumerate}
    \item Represent an individual as a matrix $M = \{m_{ij}\}$ where $m_{ij} = 1$ if and only if
          city $j$ is visited immediately after city $i$. That is, a valid tour is represented by a
          matrix with exactly one $1$ in every row and in every column.
\end{enumerate}

\paragraph{Adjacency List}
An uncommon representation is that of an adjacency list, where city $j$ occurs in the $i$th
position of the list if and only if the tour leads from city $i$ to city $j$. However, the proposed
mutation and recombination operators good subpaths in parents, so this representation is not widely
used.

\paragraph{Ordered Path}
As mentioned above, this is the most obvious representation, and the one~\cite{tsp_ea} exclusively
deals with. Here, a path is represented as an ordered list
\[C_1, C_2, \dots C_n \]
where $C_i$ occurs at the $j$th position if it is the $j$th city to be visited.

This representation yields itself nicely to the fitness evaluation of individuals in your
population, and is useful as the final representation of a returned solution. However, the
classical mutation and recombination operators typically performed on bitstrings will not work on
this representation because they can (will) result in invalid paths.

This will be the representation I use, and is thus the only representation I attempt to define
mutation and (in \autoref{prob:3}) recombination operators on. In this representation, we can
generate a starting collection of cities with

\begin{minted}{python}
    def generate_cities(n, scale=100):
        """Generates an array of cities of the given size.

        Pick random coordinates in the `scale`x`scale` grid uniformly.

        :param n: The size of the individual to generate.
        :param scale: How much to scale the individual's coordinates by.
        """
        # Scale up the uniform values from [0, 1].
        return np.random.rand(n, 2) * scale
\end{minted}

and a random population of different orderings for a collection of cities with

\begin{minted}{python}
    def generate_population(cities, size, scale=100):
        """Generate a population of individuals with the given size.

        Each individual is an array of indices into the given cities array.

        :param cities: An unsorted array of city locations.
        :param size: The number of individuals to generate.
        :param scale: How much to scale the individual's coordinates by.
        """
        n = len(cities)
        population = np.zeros((size, n), dtype=int)
        for i in range(size):
            individual = np.arange(n, dtype=int)
            np.random.shuffle(individual)
            population[i] = individual
        return population
\end{minted}

Note that in this representation, an individual is a list of indices into the
\mintinline{python}{cities} array. This is to prevent repeatedly duplicating the array of the same
cities in different orders.

The \mintinline{python}{generate_individual()} function generates a single individual as shown in
\autoref{fig:prob2:random-individual}.

\begin{figure}[h]
    \centering
    \includegraphics{prob2/figures/prob2-random-individual.pdf}
    \caption{A randomly generated individual}\label{fig:prob2:random-individual}
\end{figure}

Note that each city is an $(x, y)$ pair so that we can more easily compute the distance between
them without explicitly saving that information in a graph datastructure.

\subsubsection{Fitness}
With this representation as an individual as an ordered list of ordered pairs, an individual's
solution is given as the sum of the pairwise distances between cities. To implement this, it is
helpful to use the
\href{https://docs.python.org/3/library/itertools.html#itertools-recipes}{itertools pairwise
    recipe} for iterating over a sequence in pairs.
\begin{minted}{python}
    import itertools

    def pairwise(iterable):
        """Iterate over the given iterable in pairs.

        pairwise([1, 2, 3, 4]) -> (1, 2), (2, 3), (3, 4)
        """
        a, b = itertools.tee(iterable)
        # Advance b one step
        next(b, None)
        return zip(a, b)
\end{minted}
Then the fitness function for an individual can be written succinctly as
\begin{minted}{python}
    def fitness(cities, path):
        """Evaluate the fitness of the given path.

        Compute the Euclidean distance between every pair of cities in the path
        and add them together.

        :param cities: The array of cities through which to compute a path.
        :param path: The path through the given cities to compute the fitness for.
        :returns: The fitness of the individual.
        """
        individual = cities[path]
        return 1 / sum(np.linalg.norm(c1 - c2) for c1, c2 in pairwise(individual))
\end{minted}

Note that the sum is inverted, because we want to \textit{minimize} the fitness. I also played
around with simply negating the fitness, which worked quite well in some cases. However, in the
stochastic selection discussed below, I wanted to further separate the good individuals from the
poor individuals. Inverting the fitness seemed to have a good result.\footnote{What happens if you
    leave the fitness alone, without negating or inverting? I couldn't even \textit{begin} to
    guess, although I'm sure a naive student might spend hours and hours wondering why their
    implementation was giving them the exact opposite of what they wanted.}

\subsubsection{Mutation Methods}\label{sec:mutation-methods}
There are a number of possible mutation operators that will work on the chosen solution
representation. Note that these implementations are sometimes not as simple as one would think,
because an array access (\mintinline{python}{v = a[i]}) returns a \textit{view} of the original
array. And when you modify a view, it modifies the original array. This has particular importance
when dealing with multidimensional arrays because the returned items are now \textit{views} rather
than direct scalar values.

\paragraph{Swap Mutation} This mutation randomly swaps two cities in the tour.
\begin{minted}{python}
    def swap_mutation(path):
        """Swap two random cities in the path.

        Returns a new mutated copy of the given array.
        """
        # Arrays are (kind of) passed by reference in Python.
        x = np.copy(path)
        # Generate two valid indices to swap.
        i = np.random.randint(0, len(x) - 2)
        j = np.random.randint(i, len(x) - 1)
        # array indexing returns views, not copies
        temp = np.copy(x[i])
        x[i] = x[j]
        x[j] = temp

        return x
\end{minted}

\paragraph{Insertion Mutation} This mutation is similar to a swap. It picks a random city, removes
it from the list, and inserts it at some randomly selected position.
\begin{minted}{python}
    def insertion_mutation(path):
        """Insert a random value in the list somewhere else in the list.

        Returns a new mutated copy of the given array.
        """
        i = np.random.randint(0, len(path) - 2)
        j = np.random.randint(i, len(path) - 1)
        # Delete the element at index j and insert it before index i.
        temp = np.delete(path, j, axis=0)
        temp = np.insert(temp, i, path[j], axis=0)

        return temp
\end{minted}

\paragraph{Displacement Mutation} This is a natural extension of the insertion mutation. Rather
than removing and inserting a single city, remove and insert an entire subpath.
\begin{minted}{python}
    def displacement_mutation(path):
        """Inserts a random subarray in the list somewhere else.

        Returns a new mutated copy of the given array.
        """
        # Pick a random subarray
        i = np.random.randint(0, len(path) - 2)
        j = np.random.randint(i, len(path) - 1)
        subarray = path[i:j]
        # Delete the given subarray
        tmp = np.delete(path, range(i, j), axis=0)
        k = np.random.randint(0, len(tmp) - 1)

        return np.insert(tmp, k, subarray, axis=0)
\end{minted}

\paragraph{Shuffle Mutation} This mutation is not used much, because it can destroy good genetic
information in a parent. It picks a random subpath in a solution and shuffles it.
\begin{minted}{python}
    def shuffle_mutation(path):
        """Shuffles a random subarray in the given path.

        Returns a new mutated copy of the given array.
        """
        # Pick a random subarray
        i = np.random.randint(0, len(path) - 2)
        j = np.random.randint(i, len(path) - 1)
        x = np.copy(path)
        np.random.shuffle(x[i:j])

        return x
\end{minted}

\paragraph{Inversion Mutation} This is the mutation used in the simulated annealing example shown
in class. It picks a random subpath in a solution, and reverses it. This is a useful operator
because it breaks only two links between cities, and leaves the rest intact. All of the other
mutations can be implemented as a sequence of inversions. A proof of this remark is given in
\autoref{app:proof}. This mutation can also be extended to act like the displacement mutation where
a random subpath is removed, inverted, and reinserted somewhere else in the tour.
\begin{minted}{python}
    def inversion_mutation(path):
        """Inverts a random subarray in the given path.

        Returns a new mutated copy of the given array.
        """
        i = np.random.randint(0, len(path) - 2)
        j = np.random.randint(i, len(path) - 1)
        x = np.copy(path)
        # Invert the subarray.
        x[i:j] = x[i:j][::-1]

        return x
\end{minted}

Given these mutation methods, I define the \mintinline{python}{mutate()} function for ease of use.

\begin{minted}{python}
    def mutate(path, method='inversion'):
        """Mutate the given individual via the given method.

        Returns a new mutated copy of the given array.

        :param method: One of 'swap', 'insertion', 'displacement',
        'shuffle', or 'inversion'. Defaults to 'inversion'.
        """
        methods = {
            'swap': swap_mutation,
            'insertion': insertion_mutation,
            'displacement': displacement_mutation,
            'shuffle': shuffle_mutation,
            'inversion': inversion_mutation,
        }
        return methods[method](path)
\end{minted}

\subsubsection{Selection Methods}
In my implementation, I will deal with two different selection methods: stochastic and
deterministic. The stochastic selection will rank the population according to their fitness and
sample the sorted population by some probability distribution where the probability of being chosen
is directly proportional to an individual's fitness. The deterministic method will rank the
population by their fitness and select the top $n$ to survive.

We can implement deterministic selection by
\begin{minted}{python}
    def deterministic_selection(population, size, func, cities):
        """Deterministically select the most fit from the given population.

        Use the given fitness function to rank the population, then pick
        the next `size` of the population to move on. This assumes that,
        for a problem without recombination, the mutated individuals have
        been mixed in with the original population.

        :param population: The population to cull.
        :param size: The desired size of the population.
        :param func: The fitness function to rank the population by.
        :param cities: The array of city locations.
        :returns: The culled population, sorted upwards in increasing fitness.
        """
        fitnesses = np.array([func(cities, p) for p in population])
        indices = np.argsort(fitnesses, axis=0)
        euthanize = len(population) - size
        return population[indices][euthanize:]
\end{minted}
stochastic selection by
\begin{minted}{python}
    def stochastic_selection(population, size, func, cities):
        """Randomly select the most fit from the given population.

        Select without replacement an individual with probability
        proportional to its fitness.

        :param population: The population to cull.
        :param size: The desired size of the population.
        :param func: The fitness function to rank the population by.
        :param cities: The array of city locations.
        :returns: The culled population, unsorted.
        """
        fitnesses = np.array([func(cities, p) for p in population])
        probabilities = fitnesses / np.sum(fitnesses)
        survivors = np.random.choice(len(population), size, replace=False, p=probabilities)
        return population[survivors]
\end{minted}
and finally, a nice helper function
\begin{minted}{python}
    def select(population, size, func, cities, method='deterministic'):
        """Select the `size` most fit from the given population.

        :param population: The population to cull.
        :param size: The desired size of the population.
        :param func: The fitness function to rank the population by.
        :param cities: The array of city locations.
        :param method: One of 'deterministic' or 'stochastic'.
        :returns: The culled population, in arbitrary order.
        """
        methods = {
            'stochastic': stochastic_selection,
            'deterministic': deterministic_selection,
        }
        return methods[method](population, size, func, cities)
\end{minted}

\todoinline{Consider modifying the stochastic selection so that it can't throw away the top few?}

\subsubsection{The Evolutionary Algorithm}
With all of the above snippets defined, implementing the evolutionary algorithm listed in
\autoref{alg:standard-ea} (without recombination) can be done simply as

\begin{minted}{python}
    def simple_ea(cities, size, func, iters, mutation='inversion', selection='deterministic'):
        """Run the standard evolutionary algorithm to solve the TSP.

        This implementation does not use recombination.

        :param cities: The array of city locations.
        :param size: The population size to use.
        :param func: The fitness function to use.
        :param iters: The number of iterations (generations) to run.
        :param mutation: The type of mutation to use. One of 'swap', 'insertion',
        'displacement', 'shuffle', or 'inversion'.
        :param selection: The type of selection to use. One of 'deterministic',
        or 'stochastic'.
        """
        n = len(cities)
        population = generate_population(cities, size)
        best_fitnesses = np.zeros(iters)
        best_individuals = np.zeros((iters, n), dtype=int)
        for i in range(iters):
            # Do not recombine population.
            mutations = np.array([mutate(p, method=mutation) for p in population], dtype=int)
            # This is a $(\mu + \lambda)$ selection.
            combined = np.concatenate((population, mutations))
            population = select(combined, size, func, cities, method=selection)
            fitnesses = np.array([func(cities, p) for p in population])
            # Record the current best individual
            best = fitnesses.argmax()
            best_fitnesses[i] = fitnesses[best]
            best_individuals[i] = population[best]

        return best_fitnesses, best_individuals
\end{minted}

Note that this implementation does not have a convergence criterion. It simply runs for the
specified number of iterations.

\subsubsection{Simulated Annealing}
Since we talked about it in class, and we essentially already have the code for this problem, I
went ahead and implemented simulated annealing by tweaking the example given in class to satisfy my
sensibilities.

\begin{minted}{python}
    def evaluate(tour):
        """Evaluate the length of the given tour.

        Compute the Euclidean distance between every pair of cities in the tour
        and add them together.

        :param tour: An array of cities, where each city is n (x, y) pair.
        """
        return sum(np.linalg.norm(c1 - c2) for c1, c2 in pairwise(tour))

    def perturb(x):
        """Perturb the given array.

        Perform a sublist inversion in the interior of the given array.

        :param x: The array to perturb. Is not modified.
        """
        # Compute two random indices, avoiding the endpoints.
        i = np.random.randint(0, len(x) - 2)
        j = np.random.randint(i, len(x) - 1)
        # Produce a copy of the given array and invert a random sublist inside.
        y = np.copy(x)
        y[i:j] = y[i:j][::-1]
        return y

    def simulated_annealing(cities, temperature=800, cooling_factor=0.001):
        """Run the simulated annealing algorithm to solve the TSP.

        :param cities: An array of (x, y) city coordinates.
        :param temperature: The starting temperature of the system, defaults to 800
        :param cooling_factor: How quickly to cool the system, defaults to 0.001
        """
        current = evaluate(cities)
        energies = [current]
        while temperature > 0.001:
            new_solution = perturb(cities)
            energy = evaluate(new_solution)
            if np.random.random() < np.exp((current - energy) / temperature)
                cities = new_solution
                current = energy
                energies.append(current)
            temperature *= 1 - cooling_factor
        return cities, np.array(energies)
\end{minted}

Note that this implementation uses the uninverted fitness function, so in order to compare apples
to pears and grapefruit to plums, the returned array of fitnesses must be inverted as well.

\subsection{Results}
In \autoref{prob:1}, each run of the hill climbing and simulated annealing functions generated the
initial random starting point. Here, however, since we're comparing the results of three different
algorithms, I generated the random cities ahead of time.

\begin{figure}[h]
    \centering
    \includegraphics{prob2/figures/prob2-city-locations.pdf}
    \caption{The city locations to use for the TSP}\label{fig:prob2:city-locations}
\end{figure}

Then I chose to use only the inversion mutation and compare stochastic and deterministic selection
methods against the solution from the simulated annealing algorithm.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{prob2/figures/prob2-fitness-stochastic.pdf}
        \caption{Fitness over time}\label{fig:prob2:stochastic-fitness}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{prob2/figures/prob2-best-stochastic.pdf}
        \caption{The best solution}\label{fig:prob2:stochastic-best}
    \end{subfigure}
    \caption{The stochastic selection operator results}\label{fig:prob2:stochastic-selection}
\end{figure}

Notice in \autoref{fig:prob2:stochastic-fitness} that the fitness of the best individual in the
population over time has a lot of variability when using stochastic selection. I attempted several
variations on the stochastic selection, and I saw results similar to this in every case. Often the
best solution was the very first one attempted, and no solution past that point improved on it. In
other cases the best fitness show no overall trend upwards or downwards.

Moving from simply negating the fitness function to \textit{minimize} the tour length to leaving it
positive and inverting it seemed to have the best results. However poor results slightly improved
are still poor results.

Changing the \mintinline{python}{numpy.random.choice} function to sample with and without
replacement did not seem to have a substantial impact on the end solution, so I chose to sample
without replacement because I'm cruel and merciless.

To fix this behavior, I think the simple implementation
\begin{minted}{python}
    def stochastic_selection(population, size, func, cities):
        """Randomly select the most fit from the given population.

        Select without replacement an individual with probability
        proportional to its fitness.

        :param population: The population to cull.
        :param size: The desired size of the population.
        :param func: The fitness function to rank the population by.
        :param cities: The array of city locations.
        :returns: The culled population, unsorted.
        """
        fitnesses = np.array([func(cities, p) for p in population])
        probabilities = fitnesses / np.sum(fitnesses)
        survivors = np.random.choice(len(population), size, replace=False, p=probabilities)
        return population[survivors]
\end{minted}
should be modified to not kill the very best individuals, or should use a probability distribution
more heavily weighted towards the top than a uniform distribution weighted by an individual's
fitness. Another option would be to decrease the randomness in the selection as time progresses, as
does simulated annealing. This, combined with some kind of convergence criterion would be the best
option, but more difficult to implement than what I'm willing to tackle with only half of my
homework done and the deadline quickly approaching.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{prob2/figures/prob2-fitness-deterministic.pdf}
        \caption{Fitness over time}\label{fig:prob2:deterministic-fitness}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{prob2/figures/prob2-best-deterministic.pdf}
        \caption{The best solution}\label{fig:prob2:deterministic-best}
    \end{subfigure}
    \caption{The deterministic selection operator results}\label{fig:prob2:deterministic-selection}
\end{figure}

Notice from \autoref{fig:prob2:deterministic-fitness} that the fitness of the best individual over
time, when using deterministic selection, grows steadily before tapering off. Also notice that it
tapers off at a value almost double that of the highest peak of
\autoref{fig:prob2:stochastic-fitness}. We can plainly see that the best individual found, shown in
\autoref{fig:prob2:deterministic-best}, is a much more reasonable solution than that of stochastic
selection displayed in \autoref{fig:prob2:stochastic-best}.

However, there are still improvements that can be picked out visually.\footnote{Again, each
    run produces different figures, so this statement was true when I wrote it.} We might be able
to improve on this solution method by using a different mutation operator, but none of the ones
listed in \autoref{sec:mutation-methods} yielded any better results.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{prob2/figures/prob2-simulated-annealing-fitness.pdf}
        \caption{}\label{fig:prob2:simulated-annealing-fitness}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{prob2/figures/prob2-simulated-annealing.pdf}
        \caption{}\label{fig:prob2:simulated-annealing-best}
    \end{subfigure}
    \caption{The simulated annealing results}\label{fig:prob2:simulated-annealing}
\end{figure}

Simulated annealing produced results similar in quality to the evolutionary algorithm, with
deterministic selection, and much better results than with the stochastic selection operator.
Further, the simulated annealing algorithm terminates \textit{much} faster than the evolutionary
algorithm. This is expected, due to the amount of work being done, because the simulated annealing
algorithm works on a single individual, while the evolutionary algorithms work on an entire
population.

There are a number of tuneable parameters that I played with. There are, in my implementation,
\begin{itemize}
    \item The transformation on the fitness function
    \item The mutation operator
    \item Deterministic/stochastic selection
    \item Population size
    \item Number of generations
\end{itemize}
The transformation of the fitness function (simple negation or inversion) \textit{seemed} to have
an effect, but nothing measured. The mutation operator had a substantial impact, with the best one
being the sublist inversion. Using a deterministic selection operator consistently yielded the best
results. Being slightly impatient, I did not experiment with very large populations or large
numbers of generations. I think there is some parallelism that I could have taken advantage of to
make it easier to wait, but moving on to \autoref{prob:3} is currently more important.

\section{The TSP with Mutation and Recombination}\label{prob:3}

\subsection{Statement}
In \autoref{prob:2} we implemented EA code to solve the Traveling Salesman Problem. In this
problem, implement recombination (crossover) in your EA. For this problem you will need to use an
encoding that prevents crossover that creates an invalid candidate. As before, compare
deterministic and stochastic selection operators.

\subsection{Method}
\todoinline{
    List recombination methods from~\cite{tsp_ea}.

    Implement crossover.
}

\subsection{Results}
\todoinline{
    Compare selection operators.

    Compare recombination operators.
}

\appendix\appendixpage{}\addappheadtotoc{}
\section{Inversion Proof}\label{app:proof}

Since the best proofs are constructive, I thought I would get meta and programmatically construct a
proof that swaps, insertions, displacements, and shuffles can be implemented as a sequence of
inversions.

\inputminted{python}{proof.py}

Unfortunately, I cannot claim to be the original author of such an excellent class of program. The
inspiration came from a fellow sufferer of topology,
\href{https://www.reddit.com/user/kwprules}{u/kwprules}, a year previous to my own suffering. The
illuminated discussion on their proof technique may be found
\href{https://www.reddit.com/r/math/comments/7gqhlc/what\_to\_say\_instead\_of\_trivially/}{here}.

\bibliographystyle{ieeetr}
\bibliography{../homework}{}

\end{document}
