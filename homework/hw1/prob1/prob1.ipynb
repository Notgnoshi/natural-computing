{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "* It is incorrect to state that the x axis of the left column is \"iteration\" because those are only the accepted points in both algorithms. I don't know what else to call it. The assignment states to plot the estimate of the maximum as a function of the iteration number. I'm filtering my domain, so my plot will look like it converges much faster.\n",
    "\n",
    "  Mention this in the report, but don't worry about fixing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "# Apparently, SNS stands for \"Samuel Norman Seaborn\", a fictional\n",
    "# character from The West Wing\n",
    "import seaborn as sns\n",
    "import sympy\n",
    "\n",
    "sns.set()\n",
    "sympy.init_printing()\n",
    "# Make the figures directory if it doesn't exist.\n",
    "Path('figures/').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"The function to evaluate.\n",
    "    \n",
    "    This function returns a sympy symbolic function, float, or np.ndarray\n",
    "    depending on the type of the input.\n",
    "    \"\"\"\n",
    "    # Use symbolic sine, pi if necessary.\n",
    "    sin, pi = (sympy.sin, sympy.pi) if isinstance(x, sympy.Symbol) else (np.sin, np.pi)\n",
    "\n",
    "    return 2 ** (-2 * ((x - 0.1) / 0.9)**2) * sin(5 * pi * x) ** 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 200)\n",
    "plt.plot(x, f(x))\n",
    "plt.title('The objective function $f(x)$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.savefig('figures/prob1-function.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually pick out the periodic extremals occuring at $\\frac{n}{5} - \\frac{1}{10}$, with the global optimum over $[0, 1]$ occuring at $\\frac{1}{10}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic and Gradient Methods\n",
    "\n",
    "yuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = sympy.Symbol('x')\n",
    "f(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = sympy.diff(f(x_), x_)\n",
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve f'(x) = 0\n",
    "sympy.solveset(fp, x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = sympy.lambdify([x_], fp, 'numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, fp(x))\n",
    "plt.title('The derivative of $f(x)$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f\\'(x)$')\n",
    "plt.savefig('figures/prob1-derivative.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sp.optimize.minimize(lambda x: -f(x), 0.19, bounds=[(0, 1)])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sp.optimize.minimize(lambda x: -f(x), 0.2, bounds=[(0, 1)])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sp.optimize.minimize(lambda x: -f(x), 0.21, bounds=[(0, 1)])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "The (default) gradient method is just as susceptible to local solutions as the hill climbing algorithm. This makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hill Climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(x, bounds, sigma):\n",
    "    \"\"\"Perturb the given value by adding zero-mean white noise.\n",
    "    \n",
    "    :param x: The value to perturb.\n",
    "    :type x: float\n",
    "    :param bounds: The lower and upper bounds on the feasible region.\n",
    "    :type bounds: a (lower, upper) tuple.\n",
    "    :param sigma: The standard deviation to use when adding white noise.\n",
    "    \"\"\"\n",
    "    m, M = bounds\n",
    "    xp = x + np.random.normal(scale=sigma)\n",
    "    while xp >= M or xp <= m:\n",
    "        xp = x + np.random.normal(scale=sigma)\n",
    "    return xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill_climbing(func, bounds, sigma, iters):\n",
    "    \"\"\"Minimize the given function using the Hill Climbing algorithm.\n",
    "    \n",
    "    Each run generates a random initial guess.\n",
    "    \n",
    "    :param func: The function to minimize.\n",
    "    :type func: a function f: float -> float\n",
    "    :param bounds: The lower and upper bounds on the feasible region.\n",
    "    :type bounds: a (lower, upper) tuple.\n",
    "    :param sigma: The standard deviation to use when perturbing the current guess.\n",
    "    :param iters: The number of iterations to run the hill climbing algorithm for.\n",
    "    :returns: The path of points visited from the initial guess to the final solution.\n",
    "    :rtype: An np.ndarray with shape (iters,)\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    path = np.zeros(iters)\n",
    "    x0 = np.random.uniform(*bounds)\n",
    "    while i < iters:\n",
    "        xp = perturb(x0, bounds, sigma)\n",
    "        if func(xp) < func(x0):\n",
    "            x0 = xp\n",
    "        path[i] = x0\n",
    "        i += 1\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "fig, axes = plt.subplots(rows, 2, figsize=(9, 9), sharey=True)\n",
    "axes = iter(axes.flatten())\n",
    "for _ in range(rows):\n",
    "    path = hill_climbing(lambda x: -f(x), bounds=(0, 1), sigma=0.1, iters=100)\n",
    "    \n",
    "    ax = next(axes)\n",
    "    \n",
    "    ax.plot(path)\n",
    "    ax.set_title('Hill Climbing Path')\n",
    "    ax.set_xlabel('iteration')\n",
    "    ax.set_ylabel('$x$')\n",
    "    \n",
    "    ax = next(axes)\n",
    "    \n",
    "    # Remove duplicate entries\n",
    "    path = np.unique(path)\n",
    "    ax.plot(x, f(x), label='$f(x)$')\n",
    "    ax.plot(path, f(path), '.', label='Points accepted')\n",
    "\n",
    "    ax.set_title('Hill Climbing Path')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$f(x)$')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/prob1-hill-climbing-results.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterated_hill_climbing(func, bounds, sigma, inner_iters, iters):\n",
    "    \"\"\"Repeatedly climb the hill to find the less-local extremum via PTSD.\n",
    "    \n",
    "    Runs the iterations in parallel with as many threads as there are processors.\n",
    "    \n",
    "    :param func: The function to minimize.\n",
    "    :type func: a function f: float -> float\n",
    "    :param bounds: The lower and upper bounds on the feasible region.\n",
    "    :type bounds: a (lower, upper) tuple.\n",
    "    :param sigma: The standard deviation to use when perturbing the current guess.\n",
    "    :param inner_iters: The number of iterations to use for each run of the algorithm.\n",
    "    :param iters: The number of times to run the algorithm.\n",
    "    :returns: An array of solutions from each run, sorted by their fitness.\n",
    "    \"\"\"\n",
    "    pool = ThreadPool(multiprocessing.cpu_count())\n",
    "    # starmap consumes the given iterable in parallel until it is exhausted, collecting the results.\n",
    "    results = pool.starmap(hill_climbing, itertools.repeat((func, bounds, sigma, inner_iters), times=iters))\n",
    "    # Each result is a full path, not the optimal value\n",
    "    optimums = [r[-1] for r in results]\n",
    "    # Sort the optimums by their fitness.\n",
    "    optimums.sort(key=func)\n",
    "    return np.array(optimums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimums = iterated_hill_climbing(lambda x: -f(x), bounds=(0, 1), sigma=0.1, inner_iters=50, iters=20)\n",
    "solution = optimums[0]\n",
    "print(solution)\n",
    "\n",
    "plt.plot(x, f(x), label='$f(x)$')\n",
    "plt.plot(solution, f(solution), 'go', label='Best solution')\n",
    "plt.plot(optimums, f(optimums), '.', alpha=0.5, label='Found optimums')\n",
    "\n",
    "plt.title('Iterated Hill Climbing Optimums')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.legend()\n",
    "plt.savefig('figures/prob1-hill-climbing-solution.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulated_annealing(func, bounds, sigma, temp, cooling_factor):\n",
    "    \"\"\"Use simulated annealing to optimize the given function.\n",
    "    \n",
    "    :param func: The function to minimize.\n",
    "    :type func: a function f: float -> float\n",
    "    :param bounds: The lower and upper bounds on the feasible region.\n",
    "    :type bounds: a (lower, upper) tuple.\n",
    "    :param sigma: The standard deviation to use when perturbing the current guess.\n",
    "    :param temp: The initial temperature of the system.\n",
    "    :param cooling_factor: How quickly the system should cool.\n",
    "    :returns: An array of points *accepted* by the random condition.\n",
    "    :rtype: a 1D np.ndarray\n",
    "    \"\"\"\n",
    "    # Pick a random starting point somewhere in the domain.\n",
    "    x0 = np.random.uniform(*bounds)\n",
    "    current = func(x0)\n",
    "    path = []\n",
    "    while temp > 0.001:\n",
    "        xp = perturb(x0, bounds, sigma)\n",
    "        potential = func(xp)\n",
    "        if potential < current or np.random.random() < np.exp((current - potential) / temp):\n",
    "            x0 = xp\n",
    "            path.append(x0)\n",
    "            current = potential\n",
    "        temp *= 1 - cooling_factor\n",
    "        \n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "fig, axes = plt.subplots(rows, 2, figsize=(9, 9), sharey=True)\n",
    "axes = iter(axes.flatten())\n",
    "for _ in range(rows):\n",
    "    path = simulated_annealing(lambda x: -f(x), bounds=(0, 1), sigma=0.1, temp=0.01, cooling_factor=0.001)\n",
    "    solution = path[-1]\n",
    "    \n",
    "    ax = next(axes)\n",
    "    \n",
    "    ax.plot(path)\n",
    "    ax.set_title('Simulated Annealing Path')\n",
    "    ax.set_xlabel('iteration')\n",
    "    ax.set_ylabel('$x$')\n",
    "    \n",
    "    ax = next(axes)\n",
    "\n",
    "    ax.plot(x, f(x), label='$f(x)$')\n",
    "    ax.plot(path, f(path), '.', label='Points accepted')\n",
    "    ax.plot(solution, f(solution), 'o', label='Optimum')\n",
    "\n",
    "    ax.set_title('Simulated Annealing Path')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$f(x)$')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/prob1-simulated-annealing-results.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterated_simulated_annealing(func, bounds, sigma, temp, cooling_factor, iters):\n",
    "    \"\"\"Repeatedly run simulated annealing to improve the quality of the results.\n",
    "    \n",
    "    Runs the iterations in parallel with as many threads as there are processors.\n",
    "    \n",
    "    :param func: The function to minimize.\n",
    "    :type func: a function f: float -> float\n",
    "    :param bounds: The lower and upper bounds on the feasible region.\n",
    "    :type bounds: a (lower, upper) tuple.\n",
    "    :param sigma: The standard deviation to use when perturbing the current guess.\n",
    "    :param temp: The initial temperature of the system.\n",
    "    :param cooling_factor: How quickly the system should cool.\n",
    "    :param iters: The number of times to run the algorithm.\n",
    "    :returns: An array of solutions from each run, sorted by their fitness.\n",
    "    \"\"\"\n",
    "    pool = ThreadPool(multiprocessing.cpu_count())\n",
    "    # starmap consumes the given iterable in parallel until it is exhausted, collecting the results.\n",
    "    results = pool.starmap(simulated_annealing, itertools.repeat((func, bounds, sigma, temp, cooling_factor), times=iters))\n",
    "    # Each result is a full path, not the optimal value\n",
    "    optimums = [r[-1] for r in results]\n",
    "    # Sort the optimums by their fitness.\n",
    "    optimums.sort(key=func)\n",
    "    return np.array(optimums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimums = iterated_simulated_annealing(lambda x: -f(x), bounds=(0, 1), sigma=0.1, temp=0.01, cooling_factor=0.001, iters=20)\n",
    "solution = optimums[0]\n",
    "print(solution)\n",
    "\n",
    "plt.plot(x, f(x), label='$f(x)$')\n",
    "plt.plot(solution, f(solution), 'go', label='Best solution')\n",
    "plt.plot(optimums, f(optimums), '.', alpha=0.5, label='Found optimums')\n",
    "\n",
    "plt.title('Iterated Simulated Annealing Optimums')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.legend()\n",
    "plt.savefig('figures/prob1-simulated-annealing-solution.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* When the simulated annealing algorithm converges, it converges closer to one of the actual peaks. The hill climbing algorithm has more noise in its convergence.\n",
    "* Simulated annealing required a lot of tuning to work, and even more tuning to work well.\n",
    "* The tunable parameters that affected the correctness of the solution the most were the standard deviation of the random noise and the cooling factor. The parameter that affected speed of convergence the most was the initial temperature.\n",
    "* The solution clusters for the simulated annealing algorithm are much tighter than those for the hill climbing algorithm.\n",
    "\n",
    "You want enough noise to explore, yet not so much that you jump all over the place. Temperatures ranging from 1000 to 0.002 (because 0.001 is the convergence criterion) worked well, but smaller temperatures converged much faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
