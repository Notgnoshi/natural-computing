{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "* Fix plot titles\n",
    "* Use right math word (optimal optimum, extremal, extremum, etc)\n",
    "* Save figures as EPS (plt.savefig)\n",
    "* It is incorrect to state that the x axis of the left column is \"iteration\" because those are only the accepted points in both algorithms. I don't know what else to call it. The assignment states to plot the estimate of the maximum as a function of the iteration number. I'm filtering my domain, so my plot will look like it converges much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "import sympy\n",
    "sympy.init_printing()\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "# Apparently, SNS stands for \"Samuel Norman Seaborn\", a fictional\n",
    "# character from The West Wing\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"The function to evaluate.\n",
    "    \n",
    "    This function returns a sympy symbolic function, float, or np.ndarray\n",
    "    depending on the type of the input.\n",
    "    \"\"\"\n",
    "    # Use symbolic sine, pi if necessary.\n",
    "    sin, pi = (sympy.sin, sympy.pi) if isinstance(x, sympy.Symbol) else (np.sin, np.pi)\n",
    "\n",
    "    return 2 ** (-2 * ((x - 0.1) / 0.9)**2) * sin(5 * pi * x) ** 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 200)\n",
    "plt.plot(x, f(x))\n",
    "plt.title('The function $f$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visually pick out the periodic extremals a $\\frac{n}{5} - \\frac{1}{10}$, with the global optimum over $[0, 1]$ occuring at $\\frac{1}{10}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic and Gradient Methods\n",
    "\n",
    "yuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = sympy.Symbol('x')\n",
    "f(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = sympy.diff(f(x_), x_)\n",
    "fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve f'(x) = 0\n",
    "sympy.solveset(fp, x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = sympy.lambdify([x_], fp, 'numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, fp(x))\n",
    "plt.title('The function $f\\'(x)$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f\\'(x)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sp.optimize.minimize(lambda x: -f(x), 0.19, bounds=[(0, 1)])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sp.optimize.minimize(lambda x: -f(x), 0.2, bounds=[(0, 1)])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sp.optimize.minimize(lambda x: -f(x), 0.21, bounds=[(0, 1)])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "The (default) gradient method is just as susceptible to local solutions as the hill climbing algorithm. This makes sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hill Climbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb(x, bounds, sigma):\n",
    "    \"\"\"Perturb the given value by adding white noise.\"\"\"\n",
    "    m, M = bounds\n",
    "    xp = x + np.random.normal(scale=sigma)\n",
    "    while xp >= M or xp <= m:\n",
    "        xp = x + np.random.normal(scale=sigma)\n",
    "    return xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill_climbing(func, bounds, sigma, iters):\n",
    "    \"\"\"Minimize the given function using the Hill Climbing algorithm.\n",
    "    \n",
    "    Each run generates a random initial guess.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    path = np.zeros(iters)\n",
    "    x0 = np.random.uniform(*bounds)\n",
    "    while i < iters:\n",
    "        xp = perturb(x0, bounds, sigma)\n",
    "        if func(xp) < func(x0):\n",
    "            x0 = xp\n",
    "        path[i] = x0\n",
    "        i += 1\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "fig, axes = plt.subplots(rows, 2, figsize=(9, 9), sharey=True)\n",
    "axes = iter(axes.flatten())\n",
    "for _ in range(rows):\n",
    "    path = hill_climbing(lambda x: -f(x), bounds=(0, 1), sigma=0.1, iters=100)\n",
    "    \n",
    "    ax = next(axes)\n",
    "    \n",
    "    ax.plot(path)\n",
    "    ax.set_title('Hill Climbing')\n",
    "    ax.set_xlabel('iteration')\n",
    "    ax.set_ylabel('$x$')\n",
    "    \n",
    "    ax = next(axes)\n",
    "    \n",
    "    # Remove duplicate entries\n",
    "    path = np.unique(path)\n",
    "    ax.plot(x, f(x), label='$f(x)$')\n",
    "    ax.plot(path, f(path), '.', label='Points accepted')\n",
    "\n",
    "    ax.set_title('Hill Climbing Path')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$f(x)$')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterated_hill_climbing(func, bounds, sigma, inner_iters, iters):\n",
    "    \"\"\"Repeatedly climb the hill to find the less-local extremum via PTSD.\"\"\"\n",
    "    pool = ThreadPool(multiprocessing.cpu_count())\n",
    "    # starmap consumes the given iterable in parallel until it is exhausted, collecting the results.\n",
    "    results = pool.starmap(hill_climbing, itertools.repeat((func, bounds, sigma, inner_iters), times=iters))\n",
    "    # Each result is a full path, not the optimal value\n",
    "    optimums = [r[-1] for r in results]\n",
    "    # Sort the optimums by their fitness.\n",
    "    optimums.sort(key=func)\n",
    "    return optimums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimums = iterated_hill_climbing(lambda x: -f(x), bounds=(0, 1), sigma=0.1, inner_iters=50, iters=20)\n",
    "# Remove duplicate entries\n",
    "optimums = np.unique(optimums)\n",
    "plt.plot(x, f(x), label='$f(x)$')\n",
    "plt.plot(optimums, f(optimums), '.', label='Found optimums')\n",
    "\n",
    "plt.title('Iterated Hill Climbing Optimums')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulated_annealing(func, bounds, sigma, temp, cooling_factor):\n",
    "    \"\"\"Use simulated annealing to optimize the given function.\"\"\"\n",
    "    # Pick a random starting point somewhere in the domain.\n",
    "    x0 = np.random.uniform(*bounds)\n",
    "    current = func(x0)\n",
    "    path = []\n",
    "    while temp > 0.001:\n",
    "        xp = perturb(x0, bounds, sigma)\n",
    "        potential = func(xp)\n",
    "        if potential < current or np.random.random() < np.exp((current - potential) / temp):\n",
    "            x0 = xp\n",
    "            path.append(x0)\n",
    "            current = potential\n",
    "        temp *= 1 - cooling_factor\n",
    "        \n",
    "    return np.array(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 3\n",
    "fig, axes = plt.subplots(rows, 2, figsize=(9, 9), sharey=True)\n",
    "axes = iter(axes.flatten())\n",
    "for _ in range(rows):\n",
    "    path = simulated_annealing(lambda x: -f(x), bounds=(0, 1), sigma=0.1, temp=0.01, cooling_factor=0.001)\n",
    "    \n",
    "    ax = next(axes)\n",
    "    \n",
    "    ax.plot(path)\n",
    "    ax.set_title('Simulated Annealing')\n",
    "    ax.set_xlabel('iteration')\n",
    "    ax.set_ylabel('$x$')\n",
    "    \n",
    "    ax = next(axes)\n",
    "\n",
    "    ax.plot(x, f(x), label='$f(x)$')\n",
    "    ax.plot(path, f(path), '.', label='Points accepted')\n",
    "    ax.plot(path[-1], f(path[-1]), 'o', label='Optimum')\n",
    "\n",
    "    ax.set_title('Simulated Annealing Path')\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$f(x)$')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterated_simulated_annealing(func, bounds, sigma, temp, cooling_factor, iters):\n",
    "    \"\"\"Repeatedly run simulated annealing in parallel\"\"\"\n",
    "    pool = ThreadPool(multiprocessing.cpu_count())\n",
    "    # starmap consumes the given iterable in parallel until it is exhausted, collecting the results.\n",
    "    results = pool.starmap(simulated_annealing, itertools.repeat((func, bounds, sigma, temp, cooling_factor), times=iters))\n",
    "    # Each result is a full path, not the optimal value\n",
    "    optimums = [r[-1] for r in results]\n",
    "    # Sort the optimums by their fitness.\n",
    "    optimums.sort(key=func)\n",
    "    return np.array(optimums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimums = iterated_simulated_annealing(lambda x: -f(x), bounds=(0, 1), sigma=0.1, temp=0.01, cooling_factor=0.001, iters=20)\n",
    "\n",
    "plt.plot(x, f(x), label='$f(x)$')\n",
    "plt.plot(optimums, f(optimums), '.', label='Found optimums')\n",
    "\n",
    "plt.title('Iterated Simulated Annealing Optimums')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "* When the simulated annealing algorithm converges, it converges closer to one of the actual peaks. The hill climbing algorithm has more noise in its convergence.\n",
    "* Simulated annealing required a lot of tuning to work, and even more tuning to work well.\n",
    "* The tunable parameters that affected the correctness of the solution the most were the standard deviation of the random noise and the cooling factor. The parameter that affected speed of convergence the most was the initial temperature.\n",
    "* The solution clusters for the simulated annealing algorithm are much tighter than those for the hill climbing algorithm.\n",
    "\n",
    "You want enough noise to explore, yet not so much that you jump all over the place. Temperatures ranging from 1000 to 0.002 (because 0.001 is the convergence criterion) worked well, but smaller temperatures converged much faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
