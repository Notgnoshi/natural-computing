\documentclass[12pt]{article}
\input{homework.sty}

\title{Homework 3}
\author{Austin Gill \\ Partner TBD}

\begin{document}
\maketitle
\begingroup
\hypersetup{linkcolor=black}
\tableofcontents
\endgroup
\newpage

\section{Ant Clustering}

\subsection{Statement}

Use ant clustering to cluster uniformly distributed red and blue objects on a grid. The grid should
be $200 \times 200$ and there should be 100 each of red and blue objects. Use 500 ants.

\subsection{Method}

The ant clustering algorithm is given by \autoref{alg:ant-clustering}, and is tunable by the
definitions of $f$, $p_p$, $p_d$, and the paremeters $iters$, $ants$, $k_1$, and $k_2$.

\begin{algorithm}
    % \begin{noindent}
    \begin{algorithmic}
        \Function{ACA}{$iters$, $ants$, $k_1$, $k_2$}
            \State{Project items onto a 2D grid}
            \State{Randomly distribute ants in unoccupied (by another ant) locations}
            \State{$t \gets 1$}
            \While{$t < iters$}
                \For{$i \in \{1, \dots, ants\}$}\IComment{For each ant}
                    \State{Let $\vec x_i$ be the item $\vec x$ in ant $i$'s cell, if any}
                    \State{Compute $f(\vec x_i)$}
                    \IComment{$f$ is the perceived fraction of items near $\vec x_i$}
                    \If{ant $i$ is unloaded and cell contains an item}
                        \State{Compute pickup probability $p_p(\vec x_i)$}
                        \IComment{Tunable with parameter $k_1$}
                        \State{Pick up item $\vec x_i$ with probability $p_p(\vec x_i)$}
                    % The dumbest token for else if I've seen so far. Even worse then elif.
                    \ElsIf{ant $i$ is loaded and cell does not contain an item}
                        \State{Compute dropoff probability $p_d(\vec x_i)$}
                        \IComment{Tunable with parameter $k_2$}
                        \State{Drop off item $\vec x_i$ with probability $p_d(\vec x_i)$}
                    \EndIf{}
                    \State{Move ant $i$ to random unoccupied (by another ant) neighbor}
                \EndFor{}
                \State{$t \gets t + 1$}
            \EndWhile{}
            \State\Return{New locations of each item}
        \EndFunction{}
    \end{algorithmic}
    % \end{noindent}
    \caption{The standard Ant Clustering Algorithm (ACA)}\label{alg:ant-clustering}
\end{algorithm}

The pickup and dropoff probabilities are typically given by
\begin{align}
    p_p(\vec x_i) & = {\left(\frac{k_1}{k_1 + f(\vec x_i)}\right)}^2\label{eq:aca:pickup} \\
    p_d(\vec x_i) & = \begin{cases}
        2 f(\vec x_i) & \text{if} f(\vec x_i) < k_2 \\
        1             & \text{otherwise}
    \end{cases}\label{eq:aca:dropoff}
\end{align}
and the perceived fraction $f$ by
\begin{equation}
    f(\vec x_i) = \begin{cases}
        \displaystyle\frac{1}{s^2} \sum_{x_j \in \text{neighbors}} \left(1 -
        \frac{\mathrm{d}(\vec{x_i}, \vec{x_j})}{\alpha}\right) & \text{if } f > 0 \\
        0                                                      & \text{otherwise}
    \end{cases}\label{eq:aca:fraction}
\end{equation}
where the sum is over the square neighborhood of configurable size around the cell occupied by
$\vec{x_i}$. The parameter $\alpha$ is a tunable parameter that determines the scale of
dissimilarity, and the distance metric $\mathrm d(\vec x, \vec y)$ is the Euclidean distance
\textit{in the original, unprojected, space}. For this problem though, we are starting with
uniformly distributed points already in 2 dimensions.\footnote{This is uncommon for ant clustering
    algorithms, but is suitable for a homework problem because the process of performing the
    projection from a possibly very high dimensional space to 2 dimensions is nontrivial.
    Especially so if the projection should retain certain characteristics of the original space.

    My current understanding though, is that even a randomish projection, or a naive application of
    a dimension reduction technique like PCA is fine, because the perceived fraction $f$ uses the
    distance metric $\mathrm{d}(\vec x, \vec y)$ in the original space, and it should be possible
    to project the found clusters in 2 dimensions back into the original space.}

However, it seems this definition is nonintuitively recursive, I believe the intent of
\autoref{eq:aca:fraction} is that shown in \autoref{eq:aca:fraction-max}.

\begin{equation}
    f(\vec x_i) = \displaystyle\max\left(\frac{1}{s^2} \sum_{x_j \in \text{neighbors}} \left(1 -
        \frac{\mathrm{d}(\vec{x_i}, \vec{x_j})}{\alpha}\right), 0
    \right)\label{eq:aca:fraction-max}
\end{equation}

Regardless, since the clustering is being performed in the same space as the objects, it is
possible to use a better perceived fraction function.

\begin{equation}
    f(\vec x_i) = \sum_{x_j \in \text{neighbors}}\bigg(\mathrm{type}(x_i) =
    \mathrm{type}(x_j)\bigg)\label{eq:aca:density}
\end{equation}

That is, to count\todo{Should this be normalized?}{} the number of objects in the neighborhood (of
configurable size) around $\vec x_i$ with the same type as $\vec x_i$.

\subsection{Results}

\todoinline{Recap simulated annealing results.}

\section{Particle Swarm Optimization}

\subsection{Statement}

Use particle swarm optimization to optimize the function

\[f(x) = 2^{-2{\left(\frac{(x - 0.1)}{0.9}\right)}^2}{\big(\sin(5\pi x)\big)}^6\]

and compare the results with those of simulated annealing from Homework 1.

\subsection{Method}

The standard particle swarm algorithm from the book is given in \autoref{alg:particle-swarm}. The
algorithm is tunable by the number of iterations, the acceleration constants $AC_1$ and $AC_2$, and
the range of allowable velocities $[v_{\min}, v_{\max}]$.

\begin{algorithm}
    % \begin{noindent}
    \begin{algorithmic}
        \Function{PS}{$iters$, $AC_1$, $AC_2$, $v_{\min}$, $v_{\max}$}
            \State{Randomly initialize swarm $\vec{x_i} \in X$}
            \State{Randomly initialize swarm velocities $\Delta\vec{x_i}$}
            \IComment{$\Delta\vec{x_i} \in [v_{\min}, v_{\max}]$}
            \State{Let each $\vec{p_i} \gets \vec{x_i}$}
            \IComment{Best historical position for each particle}
            \State{$t \gets 1$}
            \While{$t < iters$}
                \For{$i \in \{1, \dots, particles\}$}
                    \IComment{For each particle}
                    \If{$f(\vec{x_i}) > f(\vec{p_i})$}
                        \IComment{Keep track of each particle's best position}
                        \State{$\vec{p_i} \gets \vec{x_i}$}
                    \EndIf{}
                    \State{$k \gets i$}\IComment{Arbitrary starting index}
                    \For{$j \in \{\text{neighbors of $\vec{x_i}$}\}$}
                        \IComment{Keep track of each particle's best neighbor}
                        \If{$f(\vec{p_j}) > g(\vec{p_k})$}
                            \State{$k \gets j$}
                        \EndIf{}
                    \EndFor{}
                    \State{Generate $\vec{\varphi_1}$ and $\vec{\varphi_2}$ uniformly}
                    \IComment{elementwise $\vec\varphi_i \in [0, AC]$}
                    % TODO: There's got to be a way to left justify the comment with the following
                    % line's indentation when it appears on an empty line...
                    \Statex\IComment{Linear combination of particle $i$'s historical best and neighbor's best}
                    \State{$\Delta\vec{x_i} \gets \Delta\vec{x_i} + \vec{\varphi_1} \otimes (\vec{p_i} - \vec{x_i}) + \vec{\varphi_2} \otimes (\vec{p_k} - \vec{x_i})$}
                    \State{$\Delta\vec{x_i} \in [v_{\min}, v_{\max}]$}
                    \IComment{Do not allow the velocities to explode, componentwise}
                    \State{$\vec{x_i} \gets \vec{x_i} + \Delta\vec{x_i}$}
                \EndFor{}
                \State{$t \gets t + 1$}
            \EndWhile{}
            \State\Return{$X$}
        \EndFunction{}
    \end{algorithmic}
    % \end{noindent}
    \caption{The standard Particle Swarm (PS) optimization algorithm}\label{alg:particle-swarm}
\end{algorithm}

The book states that the acceleration constants should equal 4.1. That is,
\[AC_1 + AC_2 = 4.1\]
with $AC_1 = AC_2 = 2.05$. The acceleration constants are used to draw the vectors of
weights $\vec{\varphi_1}$ and $\vec{\varphi_2}$ from a uniform distribution of $[0, AC_1]$ and $[0,
            AC_2]$ respectively. Note that the $\otimes$ symbol is elementwise multiplication.

The update step
\[\Delta\vec{x_i} \gets \Delta\vec{x_i} + \vec{\varphi_1} \otimes (\vec{p_i} - \vec{x_i}) +
    \vec{\varphi_2} \otimes (\vec{p_k} - \vec{x_i})\]
considers the direction towards the particle's best known position, and the best known position of
its neighbors. It updates its velocity as a randomly weighted linear combination of those
directions.

There are several variants of \autoref{alg:particle-swarm} to consider. One such variation is to
consider, not the each best historical position of each particle's neighbors, but the best
historical position of the entire swarm. This difference is shown in
\autoref{alg:particle-swarm-variant}

\begin{algorithm}
    % \begin{noindent}
    \begin{algorithmic}
        \Function{PS-variant}{$iters$, $AC_1$, $AC_2$, $v_{\min}$, $v_{\max}$}
            \State{Randomly initialize swarm $\vec{x_i} \in X$}
            \State{Randomly initialize swarm velocities $\Delta\vec{x_i}$}
            \IComment{$\Delta\vec{x_i} \in [v_{\min}, v_{\max}]$}
            \State{Let each $\vec{p_i} \gets \vec{x_i}$}
            \IComment{Best historical position for each particle}
            \State{Let $\vec{p} \gets$ best $\vec{x_i} \in X$}
            \IComment{Swarm's best historical position}
            \State{$t \gets 1$}
            \While{$t < iters$}
                \For{$i \in \{1, \dots, particles\}$}
                    \IComment{For each particle}
                    \If{$f(\vec{x_i}) > f(\vec{p_i})$}
                        \IComment{Keep track of each particle's best position}
                        \State{$\vec{p_i} \gets \vec{x_i}$}
                    \EndIf{}
                    \If{$f(\vec{x_i}) > f(\vec{p})$}
                        \IComment{Update the swarm's best position}
                        \State{$\vec{p} \gets \vec{x_i}$}
                    \EndIf{}
                    \State{Generate $\vec{\varphi_1}$ and $\vec{\varphi_2}$ uniformly}
                    \IComment{elementwise $\vec\varphi_i \in [0, AC]$}
                    % TODO: There's got to be a way to left justify the comment with the following
                    % line's indentation when it appears on an empty line...
                    \Statex\IComment{Linear combination of particle $i$'s historical best and
                        swarm's best}
                    \State{$\Delta\vec{x_i} \gets \Delta\vec{x_i} + \vec{\varphi_1} \otimes (\vec{p_i} - \vec{x_i}) + \vec{\varphi_2} \otimes (\vec{p} - \vec{x_i})$}
                    \State{$\Delta\vec{x_i} \in [v_{\min}, v_{\max}]$}
                    \IComment{Do not allow the velocities to explode, componentwise}
                    \State{$\vec{x_i} \gets \vec{x_i} + \Delta\vec{x_i}$}
                \EndFor{}
                \State{$t \gets t + 1$}
            \EndWhile{}
            \State\Return{$X$}
        \EndFunction{}
    \end{algorithmic}
    % \end{noindent}
    \caption{A variant Particle Swarm optimization algorithm}\label{alg:particle-swarm-variant}
\end{algorithm}

\todoinline{Determine which of \autoref{alg:particle-swarm}, \autoref{alg:particle-swarm-variant},
    or something else to use}

\end{document}
